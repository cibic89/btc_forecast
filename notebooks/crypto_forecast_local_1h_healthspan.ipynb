{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a0e95c-10c1-44d5-86d6-7b7e3efad0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FutureWarning for TimeSeriesScorer prediction_length suppressed.\n",
      "Predictor path already exists warning suppressed.\n"
     ]
    }
   ],
   "source": [
    "# @title Import Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines # Needed for custom legend handles\n",
    "import seaborn as sns             # Needed for plotting\n",
    "import os\n",
    "from joblib import Parallel, delayed # <<< ADD FOR PARALLELISM\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from autogluon.common import space\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import display\n",
    "import time\n",
    "import shutil\n",
    "import gc\n",
    "import pyarrow\n",
    "import warnings\n",
    "\n",
    "# Ignore the specific FutureWarning related to TimeSeriesScorer prediction_length\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated.*\", # Match start of message\n",
    "    category=FutureWarning,\n",
    "    module=\"autogluon.timeseries.metrics.abstract\" # Be specific about the source module\n",
    ")\n",
    "print(\"FutureWarning for TimeSeriesScorer prediction_length suppressed.\")\n",
    "\n",
    "# Also ignore the \"path already exists\" warning from Predictor init\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"path already exists!.*\", # Match the start of the warning message\n",
    "    category=UserWarning # This is often a UserWarning, adjust if necessary\n",
    ")\n",
    "print(\"Predictor path already exists warning suppressed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e78ac0-c476-456a-88fd-ad565d1a5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max concurrent runs: 7\n",
      "Base results directory for this run: G:\\My Drive\\code_projects\\btc_forecast\\results\\1h_20250506\n",
      "Looking for datasets in: 'G:\\My Drive\\code_projects\\btc_forecast\\datasets'\n",
      "Script frequency set to: '1h'\n",
      "\n",
      "Selected Models: ['AutoETS']\n",
      "\n",
      "Configuration set and stored in config_params.\n"
     ]
    }
   ],
   "source": [
    "# @title Base Configuration (User Parameters, Paths)\n",
    "\n",
    "# ----- Define LOCAL Paths -----\n",
    "# Use raw strings (r\"...\") for Windows paths\n",
    "local_base_path = r\"G:\\My Drive\\code_projects\\btc_forecast\" # <<< UPDATE TO YOUR LOCAL BASE PATH\n",
    "\n",
    "# --- Directory Paths ---\n",
    "datasets_dir = os.path.join(local_base_path, 'datasets')     # Directory containing .feather files\n",
    "results_base_dir = os.path.join(local_base_path, 'results') # Top-level results directory\n",
    "\n",
    "# --- Parallelism Control ---\n",
    "num_concurrent_runs = 7 # <<< SET MAX NUMBER OF PARALLEL ASSET RUNS\n",
    "print(f\"Setting max concurrent runs: {num_concurrent_runs}\")\n",
    "\n",
    "# --- Frequency & Date for Results ---\n",
    "frequency = '1h' # Explicitly set frequency for filtering files and naming\n",
    "date_today = pd.Timestamp.now().strftime(\"%Y%m%d\") # Use YYYYMMDD format\n",
    "results_dir_timedated = os.path.join(results_base_dir, f\"{frequency}_{date_today}\") # e.g., results/15m_20250506\n",
    "\n",
    "try:\n",
    "    os.makedirs(results_dir_timedated, exist_ok=True)\n",
    "    print(f\"Base results directory for this run: {results_dir_timedated}\")\n",
    "except OSError as e:\n",
    "     print(f\"Warning: Could not create results directory {results_dir_timedated}: {e}\")\n",
    "\n",
    "print(f\"Looking for datasets in: '{datasets_dir}'\")\n",
    "print(f\"Script frequency set to: '{frequency}'\")\n",
    "\n",
    "# --- Column Names ---\n",
    "timestamp_col = 'date'\n",
    "potential_target_cols = ['volume', 'open', 'high', 'low', 'close']\n",
    "all_value_cols = [timestamp_col] + potential_target_cols\n",
    "\n",
    "# --- User-defined Experiment Parameters ---\n",
    "prediction_length = 1\n",
    "max_train_dur = 2 * (60*60) # Reduced for local testing? Adjust as needed.\n",
    "desired_windows = 10\n",
    "backtest_strategy = \"rolling\"\n",
    "error_metric = \"WQL\"\n",
    "desired_quantiles = [0.05, 0.5, 0.95]\n",
    "\n",
    "# --- Model Selection ---\n",
    "full_hp = {\"AutoETS\": {}}\n",
    "selected_algorithms = [\"AutoETS\"]\n",
    "selected_hp = {alg: full_hp[alg] for alg in selected_algorithms if alg in full_hp}\n",
    "print(\"\\nSelected Models:\", selected_algorithms)\n",
    "\n",
    "# === Experiment Setup ===\n",
    "candle_counts_to_test = [100, 250, 500, 750, 1000, 1500, 2000]\n",
    "targets_to_process = potential_target_cols\n",
    "\n",
    "# ----- Healthspan Evaluation Parameters -----\n",
    "enable_healthspan_evaluation = True\n",
    "evaluation_chunk_size = 50\n",
    "max_evaluation_chunks = 20\n",
    "healthspan_threshold_std_devs = 1.5\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option('display.precision', 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# --- Store config in a dictionary for passing to parallel function ---\n",
    "config_params = {\n",
    "    \"datasets_dir\": datasets_dir,\n",
    "    \"results_dir_timedated\": results_dir_timedated,\n",
    "    \"frequency\": frequency,\n",
    "    \"date_today\": date_today,\n",
    "    \"timestamp_col\": timestamp_col,\n",
    "    \"potential_target_cols\": potential_target_cols,\n",
    "    \"all_value_cols\": all_value_cols,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"max_train_dur\": max_train_dur,\n",
    "    \"desired_windows\": desired_windows,\n",
    "    \"backtest_strategy\": backtest_strategy,\n",
    "    \"error_metric\": error_metric,\n",
    "    \"desired_quantiles\": desired_quantiles,\n",
    "    \"selected_hp\": selected_hp,\n",
    "    \"candle_counts_to_test\": candle_counts_to_test,\n",
    "    \"targets_to_process\": targets_to_process,\n",
    "    \"enable_healthspan_evaluation\": enable_healthspan_evaluation,\n",
    "    \"evaluation_chunk_size\": evaluation_chunk_size,\n",
    "    \"max_evaluation_chunks\": max_evaluation_chunks,\n",
    "    \"healthspan_threshold_std_devs\": healthspan_threshold_std_devs,\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration set and stored in config_params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae51e7-fa14-44f5-85d4-2827a72e8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper Functions (Serializer & Healthspan Calc)\n",
    "\n",
    "# --- Define serializer function ---\n",
    "def default_serializer(obj):\n",
    "    if isinstance(obj, (np.integer, np.floating, np.bool_)): return obj.item()\n",
    "    elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    elif isinstance(obj, pd.Timestamp): return obj.isoformat()\n",
    "    elif isinstance(obj, pd.Timedelta):\n",
    "         if pd.isna(obj): return None # Handle NaT\n",
    "         return obj.total_seconds()\n",
    "    elif hasattr(obj, 'get_hyperparameters') :\n",
    "        try: return obj.get_hyperparameters()\n",
    "        except Exception: return str(obj)\n",
    "    elif isinstance(obj, (dict, list, str, int, float, bool, type(None))): return obj\n",
    "    else: return str(obj)\n",
    "\n",
    "\n",
    "def calculate_healthspan(wql_scores, initial_wql, threshold_std_devs, chunk_size):\n",
    "    \"\"\" Calculates healthspan based on when WQL crosses a threshold. \"\"\"\n",
    "    if not wql_scores or initial_wql is None or pd.isna(initial_wql):\n",
    "        # print(\"  [Healthspan Calc] Cannot calculate: Invalid inputs.\") # Less verbose\n",
    "        return None, None\n",
    "    valid_scores_list = [float(s) for s in wql_scores if isinstance(s, (int, float, np.number)) and pd.notna(s)]\n",
    "    if not valid_scores_list:\n",
    "         # print(\"  [Healthspan Calc] Cannot calculate: No valid numeric scores found.\") # Less verbose\n",
    "         return None, None\n",
    "    scores_array = np.array(valid_scores_list)\n",
    "    wql_std_dev = 0\n",
    "    if len(scores_array) > 1: wql_std_dev = np.std(scores_array)\n",
    "    # elif len(scores_array) == 1: print(\"  [Healthspan Calc] Warning: Only 1 valid score...\") # Less verbose\n",
    "\n",
    "    actual_wql_baseline = -initial_wql\n",
    "    threshold = actual_wql_baseline + abs(threshold_std_devs * wql_std_dev)\n",
    "    # print(f\"  [Healthspan Calc] Initial Score (-WQL): {initial_wql:.4f}, Actual WQL Baseline: {actual_wql_baseline:.4f}, Std Dev (WQL): {wql_std_dev:.4f}, Threshold (WQL): {threshold:.4f}\") # Less verbose\n",
    "\n",
    "    healthspan_steps = None\n",
    "    for i, score in enumerate(wql_scores):\n",
    "        if pd.isna(score): continue\n",
    "        if float(score) > threshold:\n",
    "            healthspan_steps = i * chunk_size\n",
    "            # print(f\"  [Healthspan Calc] Threshold crossed at chunk {i+1} (Score: {float(score):.4f}). Estimated healthspan: {healthspan_steps} steps.\") # Less verbose\n",
    "            break\n",
    "    if healthspan_steps is None and wql_scores:\n",
    "         # print(f\"  [Healthspan Calc] Threshold not crossed within {len(wql_scores)} evaluated chunks.\") # Less verbose\n",
    "         healthspan_steps = len(wql_scores) * chunk_size\n",
    "\n",
    "    return healthspan_steps, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f44e3d4-dadb-45c3-9bc9-e1a85175cc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 512 asset files to process:\n",
      " - 1000APU_USDT_USDT-1h-futures.feather\n",
      " - 1000BONK_USDT_USDT-1h-futures.feather\n",
      " - 1000BTT_USDT_USDT-1h-futures.feather\n",
      " - 1000CAT_USDT_USDT-1h-futures.feather\n",
      " - 1000CATS_USDT_USDT-1h-futures.feather\n",
      " - 1000FLOKI_USDT_USDT-1h-futures.feather\n",
      " - 1000LUNC_USDT_USDT-1h-futures.feather\n",
      " - 1000MUMU_USDT_USDT-1h-futures.feather\n",
      " - 1000NEIROCTO_USDT_USDT-1h-futures.feather\n",
      " - 1000PEPE_USDT_USDT-1h-futures.feather\n",
      " - 1000RATS_USDT_USDT-1h-futures.feather\n",
      " - 1000TOSHI_USDT_USDT-1h-futures.feather\n",
      " - 1000TURBO_USDT_USDT-1h-futures.feather\n",
      " - 1000X_USDT_USDT-1h-futures.feather\n",
      " - 1000XEC_USDT_USDT-1h-futures.feather\n",
      " - 10000COQ_USDT_USDT-1h-futures.feather\n",
      " - 10000ELON_USDT_USDT-1h-futures.feather\n",
      " - 10000LADYS_USDT_USDT-1h-futures.feather\n",
      " - 10000QUBIC_USDT_USDT-1h-futures.feather\n",
      " - 10000SATS_USDT_USDT-1h-futures.feather\n",
      " - 10000WEN_USDT_USDT-1h-futures.feather\n",
      " - 10000WHY_USDT_USDT-1h-futures.feather\n",
      " - 1000000BABYDOGE_USDT_USDT-1h-futures.feather\n",
      " - 1000000CHEEMS_USDT_USDT-1h-futures.feather\n",
      " - 1000000MOG_USDT_USDT-1h-futures.feather\n",
      " - 1000000PEIPEI_USDT_USDT-1h-futures.feather\n",
      " - A8_USDT_USDT-1h-futures.feather\n",
      " - AAVE_USDT_USDT-1h-futures.feather\n",
      " - ACE_USDT_USDT-1h-futures.feather\n",
      " - ACH_USDT_USDT-1h-futures.feather\n",
      " - ACT_USDT_USDT-1h-futures.feather\n",
      " - ACX_USDT_USDT-1h-futures.feather\n",
      " - ADA_USDT_USDT-1h-futures.feather\n",
      " - AERGO_USDT_USDT-1h-futures.feather\n",
      " - AERO_USDT_USDT-1h-futures.feather\n",
      " - AEVO_USDT_USDT-1h-futures.feather\n",
      " - AGI_USDT_USDT-1h-futures.feather\n",
      " - AGLD_USDT_USDT-1h-futures.feather\n",
      " - AI_USDT_USDT-1h-futures.feather\n",
      " - AI16Z_USDT_USDT-1h-futures.feather\n",
      " - AIOZ_USDT_USDT-1h-futures.feather\n",
      " - AIXBT_USDT_USDT-1h-futures.feather\n",
      " - AKT_USDT_USDT-1h-futures.feather\n",
      " - ALCH_USDT_USDT-1h-futures.feather\n",
      " - ALEO_USDT_USDT-1h-futures.feather\n",
      " - ALGO_USDT_USDT-1h-futures.feather\n",
      " - ALICE_USDT_USDT-1h-futures.feather\n",
      " - ALPACA_USDT_USDT-1h-futures.feather\n",
      " - ALPHA_USDT_USDT-1h-futures.feather\n",
      " - ALT_USDT_USDT-1h-futures.feather\n",
      " - ALU_USDT_USDT-1h-futures.feather\n",
      " - ANIME_USDT_USDT-1h-futures.feather\n",
      " - ANKR_USDT_USDT-1h-futures.feather\n",
      " - APE_USDT_USDT-1h-futures.feather\n",
      " - API3_USDT_USDT-1h-futures.feather\n",
      " - APT_USDT_USDT-1h-futures.feather\n",
      " - AR_USDT_USDT-1h-futures.feather\n",
      " - ARB_USDT_USDT-1h-futures.feather\n",
      " - ARC_USDT_USDT-1h-futures.feather\n",
      " - ARK_USDT_USDT-1h-futures.feather\n",
      " - ARKM_USDT_USDT-1h-futures.feather\n",
      " - ARPA_USDT_USDT-1h-futures.feather\n",
      " - ASTR_USDT_USDT-1h-futures.feather\n",
      " - ATA_USDT_USDT-1h-futures.feather\n",
      " - ATH_USDT_USDT-1h-futures.feather\n",
      " - ATOM_USDT_USDT-1h-futures.feather\n",
      " - AUCTION_USDT_USDT-1h-futures.feather\n",
      " - AUDIO_USDT_USDT-1h-futures.feather\n",
      " - AVA_USDT_USDT-1h-futures.feather\n",
      " - AVAAI_USDT_USDT-1h-futures.feather\n",
      " - AVAIL_USDT_USDT-1h-futures.feather\n",
      " - AVAX_USDT_USDT-1h-futures.feather\n",
      " - AVL_USDT_USDT-1h-futures.feather\n",
      " - AXL_USDT_USDT-1h-futures.feather\n",
      " - AXS_USDT_USDT-1h-futures.feather\n",
      " - B3_USDT_USDT-1h-futures.feather\n",
      " - BABY_USDT_USDT-1h-futures.feather\n",
      " - BADGER_USDT_USDT-1h-futures.feather\n",
      " - BAKE_USDT_USDT-1h-futures.feather\n",
      " - BAL_USDT_USDT-1h-futures.feather\n",
      " - BAN_USDT_USDT-1h-futures.feather\n",
      " - BANANA_USDT_USDT-1h-futures.feather\n",
      " - BANANAS31_USDT_USDT-1h-futures.feather\n",
      " - BAND_USDT_USDT-1h-futures.feather\n",
      " - BAT_USDT_USDT-1h-futures.feather\n",
      " - BB_USDT_USDT-1h-futures.feather\n",
      " - BCH_USDT_USDT-1h-futures.feather\n",
      " - BEAM_USDT_USDT-1h-futures.feather\n",
      " - BEL_USDT_USDT-1h-futures.feather\n",
      " - BERA_USDT_USDT-1h-futures.feather\n",
      " - BICO_USDT_USDT-1h-futures.feather\n",
      " - BIGTIME_USDT_USDT-1h-futures.feather\n",
      " - BIO_USDT_USDT-1h-futures.feather\n",
      " - BLAST_USDT_USDT-1h-futures.feather\n",
      " - BLUR_USDT_USDT-1h-futures.feather\n",
      " - BMT_USDT_USDT-1h-futures.feather\n",
      " - BNB_USDT_USDT-1h-futures.feather\n",
      " - BNT_USDT_USDT-1h-futures.feather\n",
      " - BOBA_USDT_USDT-1h-futures.feather\n",
      " - BOME_USDT_USDT-1h-futures.feather\n",
      " - BR_USDT_USDT-1h-futures.feather\n",
      " - BRETT_USDT_USDT-1h-futures.feather\n",
      " - BROCCOLI_USDT_USDT-1h-futures.feather\n",
      " - BSV_USDT_USDT-1h-futures.feather\n",
      " - BSW_USDT_USDT-1h-futures.feather\n",
      " - BTC_USDT_USDT-1h-futures.feather\n",
      " - C98_USDT_USDT-1h-futures.feather\n",
      " - CAKE_USDT_USDT-1h-futures.feather\n",
      " - CARV_USDT_USDT-1h-futures.feather\n",
      " - CATI_USDT_USDT-1h-futures.feather\n",
      " - CELO_USDT_USDT-1h-futures.feather\n",
      " - CELR_USDT_USDT-1h-futures.feather\n",
      " - CETUS_USDT_USDT-1h-futures.feather\n",
      " - CFX_USDT_USDT-1h-futures.feather\n",
      " - CGPT_USDT_USDT-1h-futures.feather\n",
      " - CHESS_USDT_USDT-1h-futures.feather\n",
      " - CHILLGUY_USDT_USDT-1h-futures.feather\n",
      " - CHR_USDT_USDT-1h-futures.feather\n",
      " - CHZ_USDT_USDT-1h-futures.feather\n",
      " - CKB_USDT_USDT-1h-futures.feather\n",
      " - CLANKER_USDT_USDT-1h-futures.feather\n",
      " - CLOUD_USDT_USDT-1h-futures.feather\n",
      " - COMP_USDT_USDT-1h-futures.feather\n",
      " - COOK_USDT_USDT-1h-futures.feather\n",
      " - COOKIE_USDT_USDT-1h-futures.feather\n",
      " - CORE_USDT_USDT-1h-futures.feather\n",
      " - COS_USDT_USDT-1h-futures.feather\n",
      " - COTI_USDT_USDT-1h-futures.feather\n",
      " - COW_USDT_USDT-1h-futures.feather\n",
      " - CPOOL_USDT_USDT-1h-futures.feather\n",
      " - CRO_USDT_USDT-1h-futures.feather\n",
      " - CRV_USDT_USDT-1h-futures.feather\n",
      " - CTC_USDT_USDT-1h-futures.feather\n",
      " - CTK_USDT_USDT-1h-futures.feather\n",
      " - CTSI_USDT_USDT-1h-futures.feather\n",
      " - CVC_USDT_USDT-1h-futures.feather\n",
      " - CVX_USDT_USDT-1h-futures.feather\n",
      " - CYBER_USDT_USDT-1h-futures.feather\n",
      " - DARK_USDT_USDT-1h-futures.feather\n",
      " - DASH_USDT_USDT-1h-futures.feather\n",
      " - DATA_USDT_USDT-1h-futures.feather\n",
      " - DBR_USDT_USDT-1h-futures.feather\n",
      " - DEEP_USDT_USDT-1h-futures.feather\n",
      " - DEGEN_USDT_USDT-1h-futures.feather\n",
      " - DENT_USDT_USDT-1h-futures.feather\n",
      " - DEXE_USDT_USDT-1h-futures.feather\n",
      " - DGB_USDT_USDT-1h-futures.feather\n",
      " - DODO_USDT_USDT-1h-futures.feather\n",
      " - DOG_USDT_USDT-1h-futures.feather\n",
      " - DOGE_USDT_USDT-1h-futures.feather\n",
      " - DOGS_USDT_USDT-1h-futures.feather\n",
      " - DOT_USDT_USDT-1h-futures.feather\n",
      " - DRIFT_USDT_USDT-1h-futures.feather\n",
      " - DUCK_USDT_USDT-1h-futures.feather\n",
      " - DUSK_USDT_USDT-1h-futures.feather\n",
      " - DYDX_USDT_USDT-1h-futures.feather\n",
      " - DYM_USDT_USDT-1h-futures.feather\n",
      " - EDU_USDT_USDT-1h-futures.feather\n",
      " - EGLD_USDT_USDT-1h-futures.feather\n",
      " - EIGEN_USDT_USDT-1h-futures.feather\n",
      " - ELX_USDT_USDT-1h-futures.feather\n",
      " - ENA_USDT_USDT-1h-futures.feather\n",
      " - ENJ_USDT_USDT-1h-futures.feather\n",
      " - ENS_USDT_USDT-1h-futures.feather\n",
      " - EOS_USDT_USDT-1h-futures.feather\n",
      " - EPIC_USDT_USDT-1h-futures.feather\n",
      " - EPT_USDT_USDT-1h-futures.feather\n",
      " - ETC_USDT_USDT-1h-futures.feather\n",
      " - ETH_USDT_USDT-1h-futures.feather\n",
      " - ETHBTC_USDT_USDT-1h-futures.feather\n",
      " - ETHFI_USDT_USDT-1h-futures.feather\n",
      " - ETHW_USDT_USDT-1h-futures.feather\n",
      " - F_USDT_USDT-1h-futures.feather\n",
      " - FARTCOIN_USDT_USDT-1h-futures.feather\n",
      " - FB_USDT_USDT-1h-futures.feather\n",
      " - FHE_USDT_USDT-1h-futures.feather\n",
      " - FIDA_USDT_USDT-1h-futures.feather\n",
      " - FIL_USDT_USDT-1h-futures.feather\n",
      " - FIO_USDT_USDT-1h-futures.feather\n",
      " - FLM_USDT_USDT-1h-futures.feather\n",
      " - FLOCK_USDT_USDT-1h-futures.feather\n",
      " - FLOW_USDT_USDT-1h-futures.feather\n",
      " - FLR_USDT_USDT-1h-futures.feather\n",
      " - FLUX_USDT_USDT-1h-futures.feather\n",
      " - FORM_USDT_USDT-1h-futures.feather\n",
      " - FORTH_USDT_USDT-1h-futures.feather\n",
      " - FOXY_USDT_USDT-1h-futures.feather\n",
      " - FTN_USDT_USDT-1h-futures.feather\n",
      " - FUEL_USDT_USDT-1h-futures.feather\n",
      " - FWOG_USDT_USDT-1h-futures.feather\n",
      " - FXS_USDT_USDT-1h-futures.feather\n",
      " - G_USDT_USDT-1h-futures.feather\n",
      " - GALA_USDT_USDT-1h-futures.feather\n",
      " - GAS_USDT_USDT-1h-futures.feather\n",
      " - GIGA_USDT_USDT-1h-futures.feather\n",
      " - GLM_USDT_USDT-1h-futures.feather\n",
      " - GLMR_USDT_USDT-1h-futures.feather\n",
      " - GMT_USDT_USDT-1h-futures.feather\n",
      " - GMX_USDT_USDT-1h-futures.feather\n",
      " - GNO_USDT_USDT-1h-futures.feather\n",
      " - GOAT_USDT_USDT-1h-futures.feather\n",
      " - GODS_USDT_USDT-1h-futures.feather\n",
      " - GOMINING_USDT_USDT-1h-futures.feather\n",
      " - GORK_USDT_USDT-1h-futures.feather\n",
      " - GPS_USDT_USDT-1h-futures.feather\n",
      " - GRASS_USDT_USDT-1h-futures.feather\n",
      " - GRIFFAIN_USDT_USDT-1h-futures.feather\n",
      " - GRT_USDT_USDT-1h-futures.feather\n",
      " - GTC_USDT_USDT-1h-futures.feather\n",
      " - GUN_USDT_USDT-1h-futures.feather\n",
      " - HAEDAL_USDT_USDT-1h-futures.feather\n",
      " - HBAR_USDT_USDT-1h-futures.feather\n",
      " - HEI_USDT_USDT-1h-futures.feather\n",
      " - HFT_USDT_USDT-1h-futures.feather\n",
      " - HIFI_USDT_USDT-1h-futures.feather\n",
      " - HIGH_USDT_USDT-1h-futures.feather\n",
      " - HIPPO_USDT_USDT-1h-futures.feather\n",
      " - HIVE_USDT_USDT-1h-futures.feather\n",
      " - HMSTR_USDT_USDT-1h-futures.feather\n",
      " - HNT_USDT_USDT-1h-futures.feather\n",
      " - HOOK_USDT_USDT-1h-futures.feather\n",
      " - HOT_USDT_USDT-1h-futures.feather\n",
      " - HPOS10I_USDT_USDT-1h-futures.feather\n",
      " - HYPE_USDT_USDT-1h-futures.feather\n",
      " - HYPER_USDT_USDT-1h-futures.feather\n",
      " - ICP_USDT_USDT-1h-futures.feather\n",
      " - ICX_USDT_USDT-1h-futures.feather\n",
      " - ID_USDT_USDT-1h-futures.feather\n",
      " - IDEX_USDT_USDT-1h-futures.feather\n",
      " - ILV_USDT_USDT-1h-futures.feather\n",
      " - IMX_USDT_USDT-1h-futures.feather\n",
      " - INIT_USDT_USDT-1h-futures.feather\n",
      " - INJ_USDT_USDT-1h-futures.feather\n",
      " - IO_USDT_USDT-1h-futures.feather\n",
      " - IOST_USDT_USDT-1h-futures.feather\n",
      " - IOTA_USDT_USDT-1h-futures.feather\n",
      " - IOTX_USDT_USDT-1h-futures.feather\n",
      " - IP_USDT_USDT-1h-futures.feather\n",
      " - J_USDT_USDT-1h-futures.feather\n",
      " - JASMY_USDT_USDT-1h-futures.feather\n",
      " - JELLYJELLY_USDT_USDT-1h-futures.feather\n",
      " - JOE_USDT_USDT-1h-futures.feather\n",
      " - JST_USDT_USDT-1h-futures.feather\n",
      " - JTO_USDT_USDT-1h-futures.feather\n",
      " - JUP_USDT_USDT-1h-futures.feather\n",
      " - KAIA_USDT_USDT-1h-futures.feather\n",
      " - KAITO_USDT_USDT-1h-futures.feather\n",
      " - KAS_USDT_USDT-1h-futures.feather\n",
      " - KAVA_USDT_USDT-1h-futures.feather\n",
      " - KDA_USDT_USDT-1h-futures.feather\n",
      " - KERNEL_USDT_USDT-1h-futures.feather\n",
      " - KMNO_USDT_USDT-1h-futures.feather\n",
      " - KNC_USDT_USDT-1h-futures.feather\n",
      " - KOMA_USDT_USDT-1h-futures.feather\n",
      " - KSM_USDT_USDT-1h-futures.feather\n",
      " - L3_USDT_USDT-1h-futures.feather\n",
      " - LDO_USDT_USDT-1h-futures.feather\n",
      " - LEVER_USDT_USDT-1h-futures.feather\n",
      " - LINK_USDT_USDT-1h-futures.feather\n",
      " - LISTA_USDT_USDT-1h-futures.feather\n",
      " - LOOKS_USDT_USDT-1h-futures.feather\n",
      " - LPT_USDT_USDT-1h-futures.feather\n",
      " - LQTY_USDT_USDT-1h-futures.feather\n",
      " - LRC_USDT_USDT-1h-futures.feather\n",
      " - LSK_USDT_USDT-1h-futures.feather\n",
      " - LTC_USDT_USDT-1h-futures.feather\n",
      " - LUMIA_USDT_USDT-1h-futures.feather\n",
      " - LUNA2_USDT_USDT-1h-futures.feather\n",
      " - MAGIC_USDT_USDT-1h-futures.feather\n",
      " - MAJOR_USDT_USDT-1h-futures.feather\n",
      " - MANA_USDT_USDT-1h-futures.feather\n",
      " - MANTA_USDT_USDT-1h-futures.feather\n",
      " - MASA_USDT_USDT-1h-futures.feather\n",
      " - MASK_USDT_USDT-1h-futures.feather\n",
      " - MAV_USDT_USDT-1h-futures.feather\n",
      " - MAVIA_USDT_USDT-1h-futures.feather\n",
      " - MBL_USDT_USDT-1h-futures.feather\n",
      " - MBOX_USDT_USDT-1h-futures.feather\n",
      " - MDT_USDT_USDT-1h-futures.feather\n",
      " - ME_USDT_USDT-1h-futures.feather\n",
      " - MELANIA_USDT_USDT-1h-futures.feather\n",
      " - MEME_USDT_USDT-1h-futures.feather\n",
      " - MEMEFI_USDT_USDT-1h-futures.feather\n",
      " - MERL_USDT_USDT-1h-futures.feather\n",
      " - METIS_USDT_USDT-1h-futures.feather\n",
      " - MEW_USDT_USDT-1h-futures.feather\n",
      " - MICHI_USDT_USDT-1h-futures.feather\n",
      " - MINA_USDT_USDT-1h-futures.feather\n",
      " - MKR_USDT_USDT-1h-futures.feather\n",
      " - MLN_USDT_USDT-1h-futures.feather\n",
      " - MNT_USDT_USDT-1h-futures.feather\n",
      " - MOBILE_USDT_USDT-1h-futures.feather\n",
      " - MOCA_USDT_USDT-1h-futures.feather\n",
      " - MOODENG_USDT_USDT-1h-futures.feather\n",
      " - MORPHO_USDT_USDT-1h-futures.feather\n",
      " - MOVE_USDT_USDT-1h-futures.feather\n",
      " - MOVR_USDT_USDT-1h-futures.feather\n",
      " - MTL_USDT_USDT-1h-futures.feather\n",
      " - MUBARAK_USDT_USDT-1h-futures.feather\n",
      " - MVL_USDT_USDT-1h-futures.feather\n",
      " - MYRIA_USDT_USDT-1h-futures.feather\n",
      " - MYRO_USDT_USDT-1h-futures.feather\n",
      " - NC_USDT_USDT-1h-futures.feather\n",
      " - NEAR_USDT_USDT-1h-futures.feather\n",
      " - NEIROETH_USDT_USDT-1h-futures.feather\n",
      " - NEO_USDT_USDT-1h-futures.feather\n",
      " - NFP_USDT_USDT-1h-futures.feather\n",
      " - NIL_USDT_USDT-1h-futures.feather\n",
      " - NKN_USDT_USDT-1h-futures.feather\n",
      " - NMR_USDT_USDT-1h-futures.feather\n",
      " - NOT_USDT_USDT-1h-futures.feather\n",
      " - NS_USDT_USDT-1h-futures.feather\n",
      " - NTRN_USDT_USDT-1h-futures.feather\n",
      " - OBT_USDT_USDT-1h-futures.feather\n",
      " - OG_USDT_USDT-1h-futures.feather\n",
      " - OGN_USDT_USDT-1h-futures.feather\n",
      " - OL_USDT_USDT-1h-futures.feather\n",
      " - OM_USDT_USDT-1h-futures.feather\n",
      " - OMG_USDT_USDT-1h-futures.feather\n",
      " - OMNI_USDT_USDT-1h-futures.feather\n",
      " - ONDO_USDT_USDT-1h-futures.feather\n",
      " - ONE_USDT_USDT-1h-futures.feather\n",
      " - ONG_USDT_USDT-1h-futures.feather\n",
      " - ONT_USDT_USDT-1h-futures.feather\n",
      " - OP_USDT_USDT-1h-futures.feather\n",
      " - ORBS_USDT_USDT-1h-futures.feather\n",
      " - ORCA_USDT_USDT-1h-futures.feather\n",
      " - ORDER_USDT_USDT-1h-futures.feather\n",
      " - ORDI_USDT_USDT-1h-futures.feather\n",
      " - OSMO_USDT_USDT-1h-futures.feather\n",
      " - OXT_USDT_USDT-1h-futures.feather\n",
      " - PARTI_USDT_USDT-1h-futures.feather\n",
      " - PAXG_USDT_USDT-1h-futures.feather\n",
      " - PEAQ_USDT_USDT-1h-futures.feather\n",
      " - PENDLE_USDT_USDT-1h-futures.feather\n",
      " - PENGU_USDT_USDT-1h-futures.feather\n",
      " - PEOPLE_USDT_USDT-1h-futures.feather\n",
      " - PERP_USDT_USDT-1h-futures.feather\n",
      " - PHA_USDT_USDT-1h-futures.feather\n",
      " - PHB_USDT_USDT-1h-futures.feather\n",
      " - PIPPIN_USDT_USDT-1h-futures.feather\n",
      " - PIXEL_USDT_USDT-1h-futures.feather\n",
      " - PLUME_USDT_USDT-1h-futures.feather\n",
      " - PNUT_USDT_USDT-1h-futures.feather\n",
      " - POL_USDT_USDT-1h-futures.feather\n",
      " - POLYX_USDT_USDT-1h-futures.feather\n",
      " - PONKE_USDT_USDT-1h-futures.feather\n",
      " - POPCAT_USDT_USDT-1h-futures.feather\n",
      " - PORTAL_USDT_USDT-1h-futures.feather\n",
      " - POWR_USDT_USDT-1h-futures.feather\n",
      " - PRCL_USDT_USDT-1h-futures.feather\n",
      " - PRIME_USDT_USDT-1h-futures.feather\n",
      " - PROM_USDT_USDT-1h-futures.feather\n",
      " - PROMPT_USDT_USDT-1h-futures.feather\n",
      " - PUFFER_USDT_USDT-1h-futures.feather\n",
      " - PUMP_USDT_USDT-1h-futures.feather\n",
      " - PUNDIX_USDT_USDT-1h-futures.feather\n",
      " - PYR_USDT_USDT-1h-futures.feather\n",
      " - PYTH_USDT_USDT-1h-futures.feather\n",
      " - QI_USDT_USDT-1h-futures.feather\n",
      " - QNT_USDT_USDT-1h-futures.feather\n",
      " - QTUM_USDT_USDT-1h-futures.feather\n",
      " - QUICK_USDT_USDT-1h-futures.feather\n",
      " - RAD_USDT_USDT-1h-futures.feather\n",
      " - RARE_USDT_USDT-1h-futures.feather\n",
      " - RAYDIUM_USDT_USDT-1h-futures.feather\n",
      " - RDNT_USDT_USDT-1h-futures.feather\n",
      " - RED_USDT_USDT-1h-futures.feather\n",
      " - RENDER_USDT_USDT-1h-futures.feather\n",
      " - REQ_USDT_USDT-1h-futures.feather\n",
      " - REX_USDT_USDT-1h-futures.feather\n",
      " - REZ_USDT_USDT-1h-futures.feather\n",
      " - RFC_USDT_USDT-1h-futures.feather\n",
      " - RIF_USDT_USDT-1h-futures.feather\n",
      " - RLC_USDT_USDT-1h-futures.feather\n",
      " - ROAM_USDT_USDT-1h-futures.feather\n",
      " - RONIN_USDT_USDT-1h-futures.feather\n",
      " - ROSE_USDT_USDT-1h-futures.feather\n",
      " - RPL_USDT_USDT-1h-futures.feather\n",
      " - RSR_USDT_USDT-1h-futures.feather\n",
      " - RSS3_USDT_USDT-1h-futures.feather\n",
      " - RUNE_USDT_USDT-1h-futures.feather\n",
      " - RVN_USDT_USDT-1h-futures.feather\n",
      " - S_USDT_USDT-1h-futures.feather\n",
      " - SAFE_USDT_USDT-1h-futures.feather\n",
      " - SAGA_USDT_USDT-1h-futures.feather\n",
      " - SAND_USDT_USDT-1h-futures.feather\n",
      " - SC_USDT_USDT-1h-futures.feather\n",
      " - SCA_USDT_USDT-1h-futures.feather\n",
      " - SCR_USDT_USDT-1h-futures.feather\n",
      " - SCRT_USDT_USDT-1h-futures.feather\n",
      " - SD_USDT_USDT-1h-futures.feather\n",
      " - SEI_USDT_USDT-1h-futures.feather\n",
      " - SEND_USDT_USDT-1h-futures.feather\n",
      " - SERAPH_USDT_USDT-1h-futures.feather\n",
      " - SFP_USDT_USDT-1h-futures.feather\n",
      " - SHELL_USDT_USDT-1h-futures.feather\n",
      " - SHIB1000_USDT_USDT-1h-futures.feather\n",
      " - SIGN_USDT_USDT-1h-futures.feather\n",
      " - SIREN_USDT_USDT-1h-futures.feather\n",
      " - SKL_USDT_USDT-1h-futures.feather\n",
      " - SLERF_USDT_USDT-1h-futures.feather\n",
      " - SLF_USDT_USDT-1h-futures.feather\n",
      " - SLP_USDT_USDT-1h-futures.feather\n",
      " - SNT_USDT_USDT-1h-futures.feather\n",
      " - SNX_USDT_USDT-1h-futures.feather\n",
      " - SOL_USDT_USDT-1h-futures.feather\n",
      " - SOLAYER_USDT_USDT-1h-futures.feather\n",
      " - SOLO_USDT_USDT-1h-futures.feather\n",
      " - SOLV_USDT_USDT-1h-futures.feather\n",
      " - SONIC_USDT_USDT-1h-futures.feather\n",
      " - SPEC_USDT_USDT-1h-futures.feather\n",
      " - SPELL_USDT_USDT-1h-futures.feather\n",
      " - SPX_USDT_USDT-1h-futures.feather\n",
      " - SSV_USDT_USDT-1h-futures.feather\n",
      " - STEEM_USDT_USDT-1h-futures.feather\n",
      " - STG_USDT_USDT-1h-futures.feather\n",
      " - STO_USDT_USDT-1h-futures.feather\n",
      " - STORJ_USDT_USDT-1h-futures.feather\n",
      " - STPT_USDT_USDT-1h-futures.feather\n",
      " - STRK_USDT_USDT-1h-futures.feather\n",
      " - STX_USDT_USDT-1h-futures.feather\n",
      " - SUI_USDT_USDT-1h-futures.feather\n",
      " - SUN_USDT_USDT-1h-futures.feather\n",
      " - SUNDOG_USDT_USDT-1h-futures.feather\n",
      " - SUPER_USDT_USDT-1h-futures.feather\n",
      " - SUSHI_USDT_USDT-1h-futures.feather\n",
      " - SWARMS_USDT_USDT-1h-futures.feather\n",
      " - SWEAT_USDT_USDT-1h-futures.feather\n",
      " - SWELL_USDT_USDT-1h-futures.feather\n",
      " - SXP_USDT_USDT-1h-futures.feather\n",
      " - SYN_USDT_USDT-1h-futures.feather\n",
      " - SYS_USDT_USDT-1h-futures.feather\n",
      " - T_USDT_USDT-1h-futures.feather\n",
      " - TAI_USDT_USDT-1h-futures.feather\n",
      " - TAIKO_USDT_USDT-1h-futures.feather\n",
      " - TAO_USDT_USDT-1h-futures.feather\n",
      " - THE_USDT_USDT-1h-futures.feather\n",
      " - THETA_USDT_USDT-1h-futures.feather\n",
      " - TIA_USDT_USDT-1h-futures.feather\n",
      " - TLM_USDT_USDT-1h-futures.feather\n",
      " - TNSR_USDT_USDT-1h-futures.feather\n",
      " - TOKEN_USDT_USDT-1h-futures.feather\n",
      " - TON_USDT_USDT-1h-futures.feather\n",
      " - TRB_USDT_USDT-1h-futures.feather\n",
      " - TRU_USDT_USDT-1h-futures.feather\n",
      " - TRUMP_USDT_USDT-1h-futures.feather\n",
      " - TRX_USDT_USDT-1h-futures.feather\n",
      " - TSTBSC_USDT_USDT-1h-futures.feather\n",
      " - TUT_USDT_USDT-1h-futures.feather\n",
      " - TWT_USDT_USDT-1h-futures.feather\n",
      " - UMA_USDT_USDT-1h-futures.feather\n",
      " - UNI_USDT_USDT-1h-futures.feather\n",
      " - USDC_USDT_USDT-1h-futures.feather\n",
      " - USDE_USDT_USDT-1h-futures.feather\n",
      " - USTC_USDT_USDT-1h-futures.feather\n",
      " - USUAL_USDT_USDT-1h-futures.feather\n",
      " - UXLINK_USDT_USDT-1h-futures.feather\n",
      " - VANA_USDT_USDT-1h-futures.feather\n",
      " - VANRY_USDT_USDT-1h-futures.feather\n",
      " - VELO_USDT_USDT-1h-futures.feather\n",
      " - VELODROME_USDT_USDT-1h-futures.feather\n",
      " - VET_USDT_USDT-1h-futures.feather\n",
      " - VIC_USDT_USDT-1h-futures.feather\n",
      " - VINE_USDT_USDT-1h-futures.feather\n",
      " - VIRTUAL_USDT_USDT-1h-futures.feather\n",
      " - VOXEL_USDT_USDT-1h-futures.feather\n",
      " - VR_USDT_USDT-1h-futures.feather\n",
      " - VRA_USDT_USDT-1h-futures.feather\n",
      " - VTHO_USDT_USDT-1h-futures.feather\n",
      " - VVV_USDT_USDT-1h-futures.feather\n",
      " - W_USDT_USDT-1h-futures.feather\n",
      " - WAL_USDT_USDT-1h-futures.feather\n",
      " - WAVES_USDT_USDT-1h-futures.feather\n",
      " - WAXP_USDT_USDT-1h-futures.feather\n",
      " - WCT_USDT_USDT-1h-futures.feather\n",
      " - WIF_USDT_USDT-1h-futures.feather\n",
      " - WLD_USDT_USDT-1h-futures.feather\n",
      " - WOO_USDT_USDT-1h-futures.feather\n",
      " - XAI_USDT_USDT-1h-futures.feather\n",
      " - XAUT_USDT_USDT-1h-futures.feather\n",
      " - XCH_USDT_USDT-1h-futures.feather\n",
      " - XCN_USDT_USDT-1h-futures.feather\n",
      " - XDC_USDT_USDT-1h-futures.feather\n",
      " - XEM_USDT_USDT-1h-futures.feather\n",
      " - XION_USDT_USDT-1h-futures.feather\n",
      " - XLM_USDT_USDT-1h-futures.feather\n",
      " - XMR_USDT_USDT-1h-futures.feather\n",
      " - XNO_USDT_USDT-1h-futures.feather\n",
      " - XRD_USDT_USDT-1h-futures.feather\n",
      " - XRP_USDT_USDT-1h-futures.feather\n",
      " - XTER_USDT_USDT-1h-futures.feather\n",
      " - XTZ_USDT_USDT-1h-futures.feather\n",
      " - XVG_USDT_USDT-1h-futures.feather\n",
      " - XVS_USDT_USDT-1h-futures.feather\n",
      " - YFI_USDT_USDT-1h-futures.feather\n",
      " - YGG_USDT_USDT-1h-futures.feather\n",
      " - ZBCN_USDT_USDT-1h-futures.feather\n",
      " - ZEC_USDT_USDT-1h-futures.feather\n",
      " - ZEN_USDT_USDT-1h-futures.feather\n",
      " - ZENT_USDT_USDT-1h-futures.feather\n",
      " - ZEREBRO_USDT_USDT-1h-futures.feather\n",
      " - ZETA_USDT_USDT-1h-futures.feather\n",
      " - ZEUS_USDT_USDT-1h-futures.feather\n",
      " - ZIL_USDT_USDT-1h-futures.feather\n",
      " - ZK_USDT_USDT-1h-futures.feather\n",
      " - ZKJ_USDT_USDT-1h-futures.feather\n",
      " - ZORA_USDT_USDT-1h-futures.feather\n",
      " - ZRC_USDT_USDT-1h-futures.feather\n",
      " - ZRO_USDT_USDT-1h-futures.feather\n",
      " - ZRX_USDT_USDT-1h-futures.feather\n",
      " - 1INCH_USDT_USDT-1h-futures.feather\n",
      "\n",
      "==================== Starting Parallel Asset Processing (7 jobs) ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done   4 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=7)]: Done  18 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed: 31.6min\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed: 42.3min\n",
      "[Parallel(n_jobs=7)]: Done  47 tasks      | elapsed: 53.3min\n",
      "[Parallel(n_jobs=7)]: Done  58 tasks      | elapsed: 67.0min\n",
      "[Parallel(n_jobs=7)]: Done  71 tasks      | elapsed: 79.5min\n",
      "[Parallel(n_jobs=7)]: Done  84 tasks      | elapsed: 93.0min\n",
      "[Parallel(n_jobs=7)]: Done  99 tasks      | elapsed: 108.0min\n",
      "[Parallel(n_jobs=7)]: Done 114 tasks      | elapsed: 124.0min\n",
      "[Parallel(n_jobs=7)]: Done 131 tasks      | elapsed: 141.5min\n",
      "[Parallel(n_jobs=7)]: Done 148 tasks      | elapsed: 158.7min\n",
      "[Parallel(n_jobs=7)]: Done 167 tasks      | elapsed: 177.1min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 199.4min\n",
      "[Parallel(n_jobs=7)]: Done 207 tasks      | elapsed: 220.3min\n",
      "[Parallel(n_jobs=7)]: Done 228 tasks      | elapsed: 242.8min\n",
      "[Parallel(n_jobs=7)]: Done 251 tasks      | elapsed: 268.2min\n",
      "[Parallel(n_jobs=7)]: Done 274 tasks      | elapsed: 292.9min\n",
      "[Parallel(n_jobs=7)]: Done 299 tasks      | elapsed: 324.3min\n",
      "[Parallel(n_jobs=7)]: Done 324 tasks      | elapsed: 351.9min\n",
      "[Parallel(n_jobs=7)]: Done 351 tasks      | elapsed: 380.1min\n",
      "[Parallel(n_jobs=7)]: Done 378 tasks      | elapsed: 410.1min\n",
      "[Parallel(n_jobs=7)]: Done 407 tasks      | elapsed: 436.4min\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed: 468.8min\n",
      "[Parallel(n_jobs=7)]: Done 467 tasks      | elapsed: 504.3min\n",
      "[Parallel(n_jobs=7)]: Done 498 tasks      | elapsed: 538.0min\n",
      "[Parallel(n_jobs=7)]: Done 512 out of 512 | elapsed: 551.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Parallel Asset Processing Finished ====================\n",
      "Total time for parallel execution: 551.74 minutes\n",
      "\n",
      "Aggregating results...\n",
      "Aggregation complete. Total results records: 17105\n",
      "Assets processed successfully (at least one target): 509\n",
      "Assets failed or skipped: 3\n",
      "\n",
      "COMBINED detailed results summary saved to JSON: G:\\My Drive\\code_projects\\btc_forecast\\results\\1h_20250506\\20250506_1h_ALL_ASSETS_TARGETS_healthspan_summary.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'local_temp_models_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 492\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNo results generated across all assets to save.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# Optional: Clean up temporary model directory base\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[43mlocal_temp_models_base\u001b[49m):\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCleaning up temporary model directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_temp_models_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    494\u001b[39m     shutil.rmtree(local_temp_models_base, ignore_errors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'local_temp_models_base' is not defined"
     ]
    }
   ],
   "source": [
    "# @title File Discovery and Main Processing Loop\n",
    "\n",
    "# --- DEFINE AND CREATE BASE TEMP MODEL DIRECTORY (ACCESSIBLE GLOBALLY WITHIN THIS CELL) ---\n",
    "local_temp_models_base = \"ag_temp_models_local_run\" # Define it here\n",
    "try:\n",
    "    # Clean up this base temp directory from previous full script runs, if it exists\n",
    "    if os.path.exists(local_temp_models_base):\n",
    "        print(f\"Removing existing base temporary model directory: {local_temp_models_base}\")\n",
    "        shutil.rmtree(local_temp_models_base)\n",
    "    os.makedirs(local_temp_models_base, exist_ok=True)\n",
    "    print(f\"Ensured base temporary model directory exists: {local_temp_models_base}\")\n",
    "except OSError as e:\n",
    "    print(f\"Warning: Could not create/clean base temporary model directory {local_temp_models_base}: {e}\")\n",
    "    # If this fails, individual model dirs might still be created in the current working dir\n",
    "    # depending on how models_dir is constructed in process_asset.\n",
    "    # For robustness, models_dir in process_asset should ensure it's a subdirectory.\n",
    "\n",
    "# --- Asset Processing Function (for Parallelism) ---\n",
    "def process_asset(asset_file_path, config):\n",
    "    \"\"\"\n",
    "    Loads, processes, trains, evaluates, and plots results for a single asset file.\n",
    "    Args:\n",
    "        asset_file_path (str): Path to the .feather asset file.\n",
    "        config (dict): Dictionary containing all configuration parameters.\n",
    "    Returns:\n",
    "        list: A list of result dictionaries for all targets/candle counts processed\n",
    "              for this asset, or an empty list if processing fails early.\n",
    "    \"\"\"\n",
    "    # Extract config parameters (ensure all needed parameters are in the config dict)\n",
    "    frequency = config[\"frequency\"]\n",
    "    timestamp_col = config[\"timestamp_col\"]\n",
    "    potential_target_cols = config[\"potential_target_cols\"]\n",
    "    all_value_cols = config[\"all_value_cols\"]\n",
    "    prediction_length = config[\"prediction_length\"]\n",
    "    max_train_dur = config[\"max_train_dur\"]\n",
    "    desired_windows = config[\"desired_windows\"]\n",
    "    backtest_strategy = config[\"backtest_strategy\"]\n",
    "    error_metric = config[\"error_metric\"]\n",
    "    desired_quantiles = config[\"desired_quantiles\"]\n",
    "    selected_hp = config[\"selected_hp\"]\n",
    "    candle_counts_to_test = config[\"candle_counts_to_test\"]\n",
    "    targets_to_process = config[\"targets_to_process\"]\n",
    "    enable_healthspan_evaluation = config[\"enable_healthspan_evaluation\"]\n",
    "    evaluation_chunk_size = config[\"evaluation_chunk_size\"]\n",
    "    max_evaluation_chunks = config[\"max_evaluation_chunks\"]\n",
    "    healthspan_threshold_std_devs = config[\"healthspan_threshold_std_devs\"]\n",
    "    results_dir_timedated = config[\"results_dir_timedated\"] # Main results dir\n",
    "    date_today = config[\"date_today\"]\n",
    "    # local_temp_models_base is now globally accessible in this cell's scope\n",
    "\n",
    "    asset_filename = os.path.basename(asset_file_path)\n",
    "    asset_results = []\n",
    "\n",
    "    print(f\"--- Starting Processing: {asset_filename} (PID: {os.getpid()}) ---\")\n",
    "\n",
    "    try:\n",
    "        asset_name_part = asset_filename.split(f'-{frequency}-futures')[0]\n",
    "        current_item_id = f\"{asset_name_part}_FUT_{frequency}\"\n",
    "        # print(f\"[{asset_name_part}] Derived item_id: {current_item_id}\") # Less verbose\n",
    "    except Exception as e:\n",
    "        print(f\"[{asset_filename}] Warning: Could not derive asset name. Skipping. Error: {e}\")\n",
    "        return asset_results\n",
    "\n",
    "    print(f\"[{asset_name_part}] Loading asset data...\")\n",
    "    try:\n",
    "        df_full_asset = pd.read_feather(asset_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{asset_name_part}] Error loading asset file: {e}. Skipping.\")\n",
    "        return asset_results\n",
    "\n",
    "    print(f\"[{asset_name_part}] Cleaning and preprocessing...\")\n",
    "    cols_to_keep_asset = [col for col in all_value_cols if col in df_full_asset.columns]\n",
    "    df_full_asset = df_full_asset[cols_to_keep_asset].copy()\n",
    "    if timestamp_col not in df_full_asset.columns: print(f\"[{asset_name_part}] Error: Timestamp column missing. Skipping.\"); return asset_results\n",
    "    asset_targets_available = [col for col in potential_target_cols if col in df_full_asset.columns]\n",
    "    if not asset_targets_available: print(f\"[{asset_name_part}] Error: No target columns found. Skipping.\"); return asset_results\n",
    "    df_full_asset[timestamp_col] = pd.to_datetime(df_full_asset[timestamp_col], errors='coerce')\n",
    "    numeric_cols_asset = asset_targets_available\n",
    "    for col in numeric_cols_asset: df_full_asset[col] = pd.to_numeric(df_full_asset[col], errors='coerce')\n",
    "    cols_to_check_na_asset = [timestamp_col] + numeric_cols_asset\n",
    "    df_full_asset = df_full_asset.dropna(subset=cols_to_check_na_asset).copy()\n",
    "\n",
    "    if not df_full_asset.empty:\n",
    "        asset_timestamp_dtype = df_full_asset[timestamp_col].dtype\n",
    "        is_tz_aware_asset = pd.api.types.is_datetime64_ns_dtype(asset_timestamp_dtype) and df_full_asset[timestamp_col].dt.tz is not None\n",
    "        if is_tz_aware_asset: current_time_compare = pd.Timestamp.now(tz=df_full_asset[timestamp_col].dt.tz)\n",
    "        else: current_time_compare = pd.Timestamp.now().tz_localize(None)\n",
    "        df_full_asset = df_full_asset.sort_values(timestamp_col).reset_index(drop=True)\n",
    "        df_full_asset = df_full_asset[df_full_asset[timestamp_col] < current_time_compare]\n",
    "\n",
    "    if df_full_asset.empty: print(f\"[{asset_name_part}] DataFrame empty after filtering. Skipping.\"); return asset_results\n",
    "    # print(f\"[{asset_name_part}] Usable data points: {len(df_full_asset)}\") # Less verbose\n",
    "\n",
    "    print(f\"[{asset_name_part}] Starting Target Loop...\")\n",
    "    asset_loop_targets = [t for t in targets_to_process if t in asset_targets_available]\n",
    "\n",
    "    for current_target_raw in asset_loop_targets:\n",
    "        target_col = current_target_raw\n",
    "        print(f\"[{asset_name_part}] --> Processing Target: {target_col}\")\n",
    "        df_experiment_base = df_full_asset.copy()\n",
    "        results_summary_target = []\n",
    "\n",
    "        valid_candle_counts_for_target = []\n",
    "        if enable_healthspan_evaluation:\n",
    "            min_eval_points = evaluation_chunk_size\n",
    "            valid_candle_counts_for_target = [c for c in candle_counts_to_test if c + min_eval_points <= len(df_experiment_base)]\n",
    "            if not valid_candle_counts_for_target: print(f\"[{asset_name_part}/{target_col}] Skipping: Not enough data for eval.\"); continue\n",
    "        else:\n",
    "            valid_candle_counts_for_target = [c for c in candle_counts_to_test if c >= prediction_length + 1]\n",
    "            if not valid_candle_counts_for_target: print(f\"[{asset_name_part}/{target_col}] Skipping: Not enough data for train.\"); continue\n",
    "\n",
    "        print(f\"[{asset_name_part}/{target_col}] Testing candle counts: {valid_candle_counts_for_target}\")\n",
    "        for candle_count in valid_candle_counts_for_target:\n",
    "            # print(f\"[{asset_name_part}/{target_col}] ---> Running Candle Count: {candle_count}\") # Less verbose\n",
    "            iteration_start_time = time.time()\n",
    "            fit_successful = False; best_score_val_internal = None; error_message = None; fit_num_val_windows = 0; train_data = None\n",
    "            predictor = None; external_eval_scores = []; healthspan_steps = None; wql_threshold = None\n",
    "            num_chunks_to_run = 0; training_data_end_time = None\n",
    "\n",
    "            # Use local_temp_models_base defined outside this function\n",
    "            path_suffix = f\"{asset_name_part}_{frequency}_{target_col}_{backtest_strategy}_{candle_count}cndl\"\n",
    "            models_dir = os.path.join(local_temp_models_base, path_suffix)\n",
    "\n",
    "            try: # Create specific temp model dir for this run\n",
    "                os.makedirs(models_dir, exist_ok=True) # It's okay if it exists due to some race, AG warns\n",
    "            except OSError as e:\n",
    "                print(f\"[{asset_name_part}/{target_col}/{candle_count}] Error creating model sub-directory {models_dir}: {e}. Skipping.\")\n",
    "                results_summary_target.append({'target': target_col,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': f'Local model dir error: {e}','time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': 0,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None})\n",
    "                continue\n",
    "\n",
    "            # Data Slicing logic (kept as is, was working)\n",
    "            df_train_filtered = pd.DataFrame(); df_eval_slice = pd.DataFrame(); num_eval_points_needed = 0\n",
    "            if enable_healthspan_evaluation:\n",
    "                max_possible_eval_points = max(0, len(df_experiment_base) - candle_count)\n",
    "                max_possible_chunks = max_possible_eval_points // evaluation_chunk_size\n",
    "                num_chunks_to_run = min(max_evaluation_chunks, max_possible_chunks)\n",
    "                num_eval_points_needed = num_chunks_to_run * evaluation_chunk_size\n",
    "                if num_chunks_to_run > 0:\n",
    "                    df_eval_slice = df_experiment_base.tail(num_eval_points_needed).copy()\n",
    "                    train_end_index = len(df_experiment_base) - num_eval_points_needed\n",
    "                    train_start_index = max(0, train_end_index - candle_count)\n",
    "                    df_train_filtered = df_experiment_base.iloc[train_start_index:train_end_index].copy()\n",
    "                else: df_train_filtered = df_experiment_base.tail(candle_count).copy()\n",
    "            else: df_train_filtered = df_experiment_base.tail(candle_count).copy()\n",
    "\n",
    "            # --- Prepare Training Data (and rest of the logic from your loop) ---\n",
    "            # ... (The entire rest of your candle_count loop logic goes here) ...\n",
    "            # ... (Make sure 'current_item_id' is used for df_train_filtered[\"item_id\"]) ...\n",
    "            # ... (All print statements inside here should ideally include asset_name_part/target_col/candle_count for clarity) ...\n",
    "\n",
    "            # --- Start of embedded logic from previous candle_count loop ---\n",
    "            num_points_in_train = len(df_train_filtered)\n",
    "            if df_train_filtered.empty or num_points_in_train < prediction_length + 1:\n",
    "                 error_msg_detail = 'Empty training slice' if df_train_filtered.empty else f'Insufficient training data ({num_points_in_train})'\n",
    "                 print(f\"[{asset_name_part}/{target_col}/{candle_count}] Error: {error_msg_detail}. Skipping.\")\n",
    "                 results_summary_target.append({'target': target_col,'asset': asset_name_part,'candle_count': candle_count,'error': error_msg_detail,'best_score_val_internal': None,'time_taken_seconds': time.time()-iteration_start_time,'num_data_points_train':0,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None})\n",
    "                 if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True); continue\n",
    "\n",
    "            df_train_filtered[\"item_id\"] = current_item_id\n",
    "            id_column_name = \"item_id\"\n",
    "\n",
    "            if df_train_filtered[timestamp_col].dt.tz is not None:\n",
    "                try: df_train_filtered[timestamp_col] = df_train_filtered[timestamp_col].dt.tz_localize(None)\n",
    "                except Exception as tz_e: print(f\"TZ Error: {tz_e}\"); results_summary_target.append({'target': target_col,'asset': asset_name_part, 'candle_count': candle_count, 'error': f'TZ conversion failed: {tz_e}', 'best_score_val_internal': None, 'time_taken_seconds': time.time()-iteration_start_time, 'num_data_points_train':num_points_in_train, 'num_val_windows_used': 0, 'external_eval_wql_scores': [], 'healthspan_steps': None, 'wql_threshold': None}); if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True); continue\n",
    "\n",
    "            training_data_end_time = df_train_filtered[timestamp_col].iloc[-1]\n",
    "\n",
    "            try:\n",
    "                required_cols_tsdf = [id_column_name, timestamp_col, target_col]\n",
    "                cols_for_tsdf = [col for col in df_train_filtered.columns if col in required_cols_tsdf or col in potential_target_cols]\n",
    "                train_data = TimeSeriesDataFrame.from_data_frame(df_train_filtered[cols_for_tsdf], id_column=id_column_name, timestamp_column=timestamp_col)\n",
    "            except Exception as e: print(f\"TSDF Error: {e}\"); error_message = f\"Train TSDF creation: {e}\"; results_summary_target.append({'target': target_col,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': error_message,'time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': num_points_in_train,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None}); if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True); continue\n",
    "\n",
    "            fit_num_val_windows = 0; fit_val_step_size = 1\n",
    "            if desired_windows > 0:\n",
    "                 total_data_len_val = len(train_data); min_train_for_val = prediction_length; required_for_val = prediction_length * desired_windows * prediction_length\n",
    "                 if total_data_len_val >= min_train_for_val + required_for_val: fit_num_val_windows = desired_windows; fit_val_step_size = prediction_length\n",
    "                 else: max_possible = (total_data_len_val - min_train_for_val) // (prediction_length * prediction_length); fit_num_val_windows = max(0, min(desired_windows, max_possible));\n",
    "                 if fit_num_val_windows > 0: fit_val_step_size = prediction_length\n",
    "\n",
    "            try: predictor = TimeSeriesPredictor(prediction_length=prediction_length, target=target_col,eval_metric=error_metric, path=models_dir, quantile_levels=desired_quantiles)\n",
    "            except Exception as e: error_message = f'Predictor init failed: {e}'; print(f\"Predictor Init Error: {e}\"); results_summary_target.append({'target': target_col,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': error_message,'time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': num_points_in_train,'num_val_windows_used': fit_num_val_windows,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None}); if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True); continue\n",
    "\n",
    "            print(f\"[{asset_name_part}/{target_col}/{candle_count}] Fitting model...\")\n",
    "            hpo_tune_kwargs = None\n",
    "            try: predictor.fit(train_data, presets=\"best_quality\", time_limit=max_train_dur, num_val_windows=fit_num_val_windows, val_step_size=fit_val_step_size, refit_full=True, hyperparameters=selected_hp, hyperparameter_tune_kwargs=hpo_tune_kwargs, enable_ensemble=False, verbosity=0); fit_successful = True\n",
    "            except Exception as fit_e: error_message = f\"Fitting failed: {fit_e}\"; print(f\"ERROR during fitting: {fit_e}\")\n",
    "\n",
    "            best_model_name = \"N/A\"\n",
    "            if fit_successful:\n",
    "                leaderboard_df = None; best_score_val_internal = None\n",
    "                try:\n",
    "                    if fit_num_val_windows > 0: leaderboard_df = predictor.leaderboard(silent=True)\n",
    "                    if leaderboard_df is not None and not leaderboard_df.empty and 'score_val' in leaderboard_df.columns and pd.api.types.is_numeric_dtype(leaderboard_df['score_val']):\n",
    "                         ets_lb = leaderboard_df[leaderboard_df['model'].str.contains(\"ETS\", na=False)]\n",
    "                         if not ets_lb.empty: score = ets_lb[\"score_val\"].iloc[0];\n",
    "                         if pd.notna(score): best_score_val_internal = score; best_model_name = ets_lb[\"model\"].iloc[0]\n",
    "                except Exception as lb_e: print(f\"Leaderboard Error: {lb_e}\"); best_score_val_internal = None\n",
    "\n",
    "            if fit_successful and enable_healthspan_evaluation and not df_eval_slice.empty and num_chunks_to_run > 0:\n",
    "                external_eval_scores = []\n",
    "                try:\n",
    "                    future_data_full = df_eval_slice; future_data_full[\"item_id\"] = current_item_id\n",
    "                    if future_data_full[timestamp_col].dt.tz is not None: future_data_full[timestamp_col] = future_data_full[timestamp_col].dt.tz_localize(None)\n",
    "                    eval_cols_for_tsdf = [col for col in future_data_full.columns if col in required_cols_tsdf or col in potential_target_cols]\n",
    "                    future_tsdf_full = TimeSeriesDataFrame.from_data_frame(future_data_full[eval_cols_for_tsdf], id_column=id_column_name, timestamp_column=timestamp_col)\n",
    "                    for i in range(num_chunks_to_run):\n",
    "                        chunk_start_idx = i * evaluation_chunk_size; chunk_end_idx = (i + 1) * evaluation_chunk_size\n",
    "                        current_chunk_data = future_tsdf_full.iloc[chunk_start_idx:chunk_end_idx]\n",
    "                        time_diff = pd.NaT\n",
    "                        try: # Time calculation\n",
    "                            chunk_index = current_chunk_data.index\n",
    "                            if not chunk_index.empty:\n",
    "                                chunk_end_timestamp_obj = chunk_index[-1][1]; chunk_end_time = pd.to_datetime(chunk_end_timestamp_obj)\n",
    "                                if not isinstance(training_data_end_time, pd.Timestamp): training_data_end_time_ts = pd.to_datetime(training_data_end_time)\n",
    "                                else: training_data_end_time_ts = training_data_end_time\n",
    "                                if pd.notna(chunk_end_time) and pd.notna(training_data_end_time_ts): time_diff = chunk_end_time - training_data_end_time_ts\n",
    "                                else: time_diff = pd.Timedelta(seconds=np.nan)\n",
    "                            else: time_diff = pd.Timedelta(seconds=np.nan)\n",
    "                        except Exception: time_diff = pd.Timedelta(seconds=np.nan)\n",
    "\n",
    "                        if len(current_chunk_data) < prediction_length + 1: external_eval_scores.append({'time_since_train': time_diff, 'WQL': np.nan}); continue\n",
    "                        try:\n",
    "                            eval_metrics = predictor.evaluate(current_chunk_data, model='AutoETS_FULL', metrics=[error_metric])\n",
    "                            wql_score_chunk = eval_metrics.get(error_metric, np.nan)\n",
    "                            external_eval_scores.append({'time_since_train': time_diff, 'WQL': wql_score_chunk})\n",
    "                        except Exception as eval_e: print(f\"ERROR evaluating chunk {i+1}: {eval_e}\"); external_eval_scores.append({'time_since_train': time_diff, 'WQL': np.nan})\n",
    "                except Exception as ext_eval_setup_e: print(f\"Ext Eval Error: {ext_eval_setup_e}\"); error_message = f\"Ext Eval Error: {ext_eval_setup_e}\" if error_message is None else error_message\n",
    "                if external_eval_scores:\n",
    "                     wql_values = [item['WQL'] for item in external_eval_scores if pd.notna(item['WQL'])]\n",
    "                     if wql_values:\n",
    "                         baseline_wql = -best_score_val_internal if (best_score_val_internal is not None and pd.notna(best_score_val_internal)) else None\n",
    "                         if baseline_wql is not None: healthspan_steps, wql_threshold = calculate_healthspan(wql_scores=wql_values, initial_wql=baseline_wql, threshold_std_devs=healthspan_threshold_std_devs, chunk_size=evaluation_chunk_size)\n",
    "\n",
    "            iteration_end_time = time.time()\n",
    "            results_summary_target.append({'target': target_col,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': best_score_val_internal,'error': error_message,'time_taken_seconds': iteration_end_time - iteration_start_time,'num_data_points_train': num_points_in_train,'num_val_windows_used': fit_num_val_windows,'external_eval_wql_scores': external_eval_scores,'healthspan_steps': healthspan_steps,'wql_threshold': wql_threshold})\n",
    "            print(f\"--- Finished Iteration: Asset={asset_name_part}, Target={target_col}, Candles={candle_count}, Time={results_summary_target[-1]['time_taken_seconds']:.1f}s, Score={best_score_val_internal if best_score_val_internal is not None else 'N/A'}, Healthspan={healthspan_steps if healthspan_steps is not None else 'N/A'} ---\")\n",
    "\n",
    "            if predictor is not None: del predictor; gc.collect()\n",
    "            if 'train_data' in locals() and train_data is not None: del train_data; gc.collect()\n",
    "            # Clean up models_dir for this specific iteration if it's temporary *per iteration*\n",
    "            if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "        # === END CANDLE COUNT LOOP (for current target/asset) ===\n",
    "        print(f\"\\n[{asset_name_part}/{target_col}] Finished Candle Count Loop.\")\n",
    "        # Append results for this target to the main list for this asset\n",
    "        asset_results.extend(results_summary_target)\n",
    "\n",
    "        # === Plotting and Saving for current target/asset ===\n",
    "        if not results_df_target.empty:\n",
    "            # (The existing plotting and saving logic for a single target/asset combination)\n",
    "            # Ensure save paths include asset_name_part and results_dir_timedated\n",
    "            # --- Display Summary Table ---\n",
    "            results_df_target_display = pd.DataFrame(results_summary_target) # Use the specific list for this target\n",
    "            display_cols = ['candle_count', 'best_score_val_internal', 'healthspan_steps', 'wql_threshold', 'error', 'time_taken_seconds']\n",
    "            display_cols_present = [col for col in display_cols if col in results_df_target_display.columns]\n",
    "            if display_cols_present: display(results_df_target_display[display_cols_present])\n",
    "            else: display(results_df_target_display)\n",
    "\n",
    "            # --- Save Detailed JSON Summary FOR THIS TARGET/ASSET ---\n",
    "            summary_filename = f'{date_today}_{asset_name_part}_{frequency}_{target_col}_{backtest_strategy}_healthspan_summary.json'\n",
    "            summary_save_path = os.path.join(results_dir_timedated, summary_filename)\n",
    "            try: # Save JSON (using the serialization helper)\n",
    "                 with open(summary_save_path, 'w') as f: json.dump(results_summary_target, f, default=default_serializer, indent=4) # Save target specific\n",
    "                 print(f\"[{asset_name_part}/{target_col}] Detailed summary saved: {summary_save_path}\")\n",
    "            except Exception as e: print(f\"[{asset_name_part}/{target_col}] Error saving JSON summary: {e}\")\n",
    "\n",
    "            # --- Prepare Data for Enhanced Plot ---\n",
    "            plot_data_list = []\n",
    "            for idx, row in results_df_target_display.iterrows(): # Use target-specific df\n",
    "                cc = row['candle_count']; oos_scores = row.get('external_eval_wql_scores', [])\n",
    "                if oos_scores:\n",
    "                    for i_chunk, score_item in enumerate(oos_scores):\n",
    "                        wql = score_item.get('WQL'); steps = (i_chunk + 0.5) * evaluation_chunk_size\n",
    "                        plot_data_list.append({'candle_count': cc,'steps_since_train': steps,'WQL': wql,'WQL_neg': -wql if pd.notna(wql) else np.nan})\n",
    "            plot_df_target_external = pd.DataFrame(plot_data_list) if plot_data_list else pd.DataFrame()\n",
    "            if not plot_df_target_external.empty: plot_df_target_external.dropna(subset=['steps_since_train', 'WQL_neg'], inplace=True)\n",
    "\n",
    "            # --- PLOT 1: Internal Score vs Candle Count ---\n",
    "            results_df_filtered_internal = results_df_target_display.dropna(subset=['best_score_val_internal']).copy()\n",
    "            if not results_df_filtered_internal.empty:\n",
    "                # (Plotting logic as before, using asset_name_part and target_col in title/filename)\n",
    "                plt.figure(figsize=(12, 6)); plt.plot(results_df_filtered_internal['candle_count'], results_df_filtered_internal['best_score_val_internal'], marker='o', linestyle='-')\n",
    "                plt.title(f'Internal Score (-WQL) vs. Candle Count (Asset: {asset_name_part}, Target: {target_col})'); plt.xlabel('Number of Candles (`candle_count`)'); plt.ylabel(f'Internal Fit Score (-WQL, Higher is Better)')\n",
    "                plt.grid(True); plt.xticks(rotation=45); plt.tight_layout()\n",
    "                plot_filename_internal = f'{date_today}_{asset_name_part}_{frequency}_{target_col}_{backtest_strategy}_internal_score_plot.png'\n",
    "                plot_save_path_internal = os.path.join(results_dir_timedated, plot_filename_internal)\n",
    "                try: plt.savefig(plot_save_path_internal, dpi=150); print(f\"[{asset_name_part}/{target_col}] Internal score plot saved: {plot_save_path_internal}\")\n",
    "                except Exception as e: print(f\"[{asset_name_part}/{target_col}] Error saving internal plot: {e}\")\n",
    "                plt.close() # Close plot\n",
    "\n",
    "            # --- PLOT 2: Enhanced Healthspan Plot ---\n",
    "            if not plot_df_target_external.empty:\n",
    "                 print(f\"[{asset_name_part}/{target_col}] Generating Enhanced Healthspan Plot...\")\n",
    "                 # (Plotting logic as before, using asset_name_part and target_col in title/filename)\n",
    "                 plot_dir_fig2 = results_dir_timedated; timestamp_str_fig2 = date_today; script_name_base_fig2 = f\"{asset_name_part}_{target_col}\"\n",
    "                 plt.style.use('seaborn-v0_8-whitegrid'); fig, ax = plt.subplots(figsize=(16, 9))\n",
    "                 candle_counts_plot = sorted(plot_df_target_external['candle_count'].unique())\n",
    "                 palette = sns.color_palette(n_colors=len(candle_counts_plot)); color_map = dict(zip(candle_counts_plot, palette))\n",
    "                 sns.lineplot(data=plot_df_target_external, x='steps_since_train', y='WQL_neg', hue='candle_count', hue_order=candle_counts_plot, palette=color_map, marker='.', linewidth=1.5, legend=False, ax=ax)\n",
    "                 is_legend_handles = [];\n",
    "                 for cc_plot in candle_counts_plot: # Use cc_plot to avoid conflict\n",
    "                     is_score_row = results_df_target_display[results_df_target_display['candle_count'] == cc_plot]\n",
    "                     if not is_score_row.empty: is_score = is_score_row['best_score_val_internal'].iloc[0]\n",
    "                     if pd.notna(is_score): ax.axhline(y=is_score, color=color_map.get(cc_plot, 'grey'), linestyle=':', linewidth=1.5, alpha=0.9); is_legend_handles.append(mlines.Line2D([], [], color=color_map.get(cc_plot, 'grey'), linestyle=':', linewidth=1.5, label=f'CC {cc_plot} (IS Ref)'))\n",
    "                 median_wql_neg = plot_df_target_external.groupby('steps_since_train')['WQL_neg'].median(); median_handle = []\n",
    "                 if not median_wql_neg.empty: ax.plot(median_wql_neg.index, median_wql_neg.values, color='black', linestyle='--', linewidth=2.5, label='Median OOS'); median_handle = [mlines.Line2D([], [], color='black', linestyle='--', linewidth=2.5, label='Median OOS')]\n",
    "                 threshold_handle = []; avg_actual_wql_threshold = results_df_target_display['wql_threshold'].mean()\n",
    "                 if pd.notna(avg_actual_wql_threshold): avg_neg_threshold_plot = -avg_actual_wql_threshold; ax.axhline(y=avg_neg_threshold_plot, color='red', linestyle='-.', linewidth=2.0, label=f'Avg Degrad. Threshold (≈ {avg_neg_threshold_plot:.4f})'); threshold_handle = [mlines.Line2D([], [], color='red', linestyle='-.', linewidth=2.0, label=f'Avg Degrad. Threshold (≈ {avg_neg_threshold_plot:.4f})')]\n",
    "                 oos_legend_handles = [mlines.Line2D([], [], color=color_map.get(cc_plot, 'grey'), marker='.', linewidth=1.5, linestyle='-', label=f'CC {cc_plot} (OOS)') for cc_plot in candle_counts_plot]\n",
    "                 all_handles = oos_legend_handles + is_legend_handles + median_handle + threshold_handle\n",
    "                 valid_handles = [h for h in all_handles if h.get_label() and not h.get_label().startswith('_')]; ax.legend(handles=valid_handles, title='Performance (-WQL, Higher is Better)', loc='best', fontsize='small')\n",
    "                 ax.set_title(f\"Model Healthspan: OOS vs IS (Asset: {asset_name_part}, Target: {target_col})\", fontsize=16)\n",
    "                 ax.set_xlabel(f\"Steps Since Training (Approx. based on Chunk Size = {evaluation_chunk_size})\", fontsize=12); ax.set_ylabel(\"Performance Score (-WQL, Higher is Better)\", fontsize=12)\n",
    "                 ax.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey'); ax.grid(True, which='minor', linestyle=':', linewidth='0.5', color='lightgrey'); ax.minorticks_on()\n",
    "                 try:\n",
    "                     plot_filename = f'{timestamp_str_fig2}_{script_name_base_fig2}_{frequency}_{backtest_strategy}_healthspan_plot_v2.png' # Corrected original variable names used here\n",
    "                     plot_save_path = os.path.join(plot_dir_fig2, plot_filename)\n",
    "                     plt.savefig(plot_save_path, bbox_inches='tight', dpi=150)\n",
    "                     print(f\"[{asset_name_part}/{target_col}] Enhanced Healthspan plot saved: {plot_save_path}\")\n",
    "                 except Exception as e: print(f\"[{asset_name_part}/{target_col}] Error saving plot: {e}\")\n",
    "                 plt.close(fig)\n",
    "            else: print(f\"\\n[{asset_name_part}/{target_col}] Skipping enhanced healthspan plot: No valid external evaluation data.\")\n",
    "        else: print(f\"\\n[{asset_name_part}/{target_col}] No valid results to plot.\")\n",
    "\n",
    "        # Cleanup for Target\n",
    "        if 'df_experiment_base' in locals(): del df_experiment_base; gc.collect()\n",
    "        if 'results_df_target_display' in locals(): del results_df_target_display; gc.collect()\n",
    "        if 'results_df_filtered_internal' in locals(): del results_df_filtered_internal; gc.collect()\n",
    "        if 'plot_df_target_external' in locals(): del plot_df_target_external; gc.collect()\n",
    "\n",
    "    # --- END TARGET LOOP (for current asset) ---\n",
    "    print(f\"\\n[{asset_name_part}] Finished Target Loop.\")\n",
    "    # asset_results now contains all results for the current asset (list of lists of dicts)\n",
    "    # For the overall list, we're extending it directly with results_summary_target,\n",
    "    # so asset_results list is not strictly needed if appending directly to all_assets_all_targets_results\n",
    "    # However, if you wanted to return it from the function:\n",
    "    # return asset_results # Or flatten it before returning\n",
    "\n",
    "# === END ASSET FILE LOOP ===\n",
    "print(f\"\\n\\n{'='*20} Finished All Asset Files {'='*20}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Re-fetch files if needed, or assume asset_files_to_process is populated\n",
    "    if not asset_files_to_process:\n",
    "        print(\"No asset files were discovered. Re-run file discovery or check paths.\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*20} Starting Parallel Asset Processing ({num_concurrent_runs} jobs) {'='*20}\")\n",
    "        start_parallel_time = time.time()\n",
    "\n",
    "        results_list_parallel = Parallel(n_jobs=num_concurrent_runs, backend=\"loky\", verbose=10)(\n",
    "             delayed(process_asset)(file_path, config_params) for file_path in asset_files_to_process\n",
    "        )\n",
    "\n",
    "        end_parallel_time = time.time()\n",
    "        print(f\"\\n{'='*20} Parallel Asset Processing Finished {'='*20}\")\n",
    "        print(f\"Total time for parallel execution: {(end_parallel_time - start_parallel_time)/60:.2f} minutes\")\n",
    "\n",
    "        # --- Flatten results from all parallel runs ---\n",
    "        print(\"\\nAggregating results from parallel jobs...\")\n",
    "        all_assets_all_targets_results_flat = []\n",
    "        job_success_count = 0; job_fail_count = 0\n",
    "        if results_list_parallel: # Check if any results came back\n",
    "            for single_asset_results_list in results_list_parallel:\n",
    "                if isinstance(single_asset_results_list, list) and single_asset_results_list: # Job returned a non-empty list\n",
    "                    all_assets_all_targets_results_flat.extend(single_asset_results_list)\n",
    "                    job_success_count +=1\n",
    "                elif isinstance(single_asset_results_list, list) and not single_asset_results_list: # Job returned empty list (e.g. asset skipped early)\n",
    "                    job_fail_count +=1 # Count as skipped/failed\n",
    "                else: # Job might have returned None or an exception object if not handled within\n",
    "                    print(f\"Warning: A parallel job did not return a list of results. Result: {type(single_asset_results_list)}\")\n",
    "                    job_fail_count +=1\n",
    "        print(f\"Aggregation complete. Total results records: {len(all_assets_all_targets_results_flat)}\")\n",
    "        print(f\"Assets processed (at least one target run): {job_success_count}\")\n",
    "        print(f\"Assets potentially failed or skipped entirely: {job_fail_count}\")\n",
    "\n",
    "        # --- Save ALL results combined ---\n",
    "        if all_assets_all_targets_results_flat:\n",
    "             all_results_save_path = os.path.join(config_params[\"results_dir_timedated\"], f'{config_params[\"date_today\"]}_{config_params[\"frequency\"]}_ALL_ASSETS_TARGETS_healthspan_summary.json')\n",
    "             try:\n",
    "                  # Use the same serialization logic as before\n",
    "                  serializable_all_results_flat = []\n",
    "                  for record in all_assets_all_targets_results_flat:\n",
    "                      new_record = record.copy(); temp_scores = []\n",
    "                      if 'external_eval_wql_scores' in new_record and new_record['external_eval_wql_scores']:\n",
    "                          for score_item in new_record['external_eval_wql_scores']:\n",
    "                               new_score_item = score_item.copy()\n",
    "                               time_delta = new_score_item.get('time_since_train')\n",
    "                               if isinstance(time_delta, pd.Timedelta): new_score_item['time_since_train'] = None if pd.isna(time_delta) else time_delta.total_seconds()\n",
    "                               temp_scores.append(new_score_item)\n",
    "                          new_record['external_eval_wql_scores'] = temp_scores\n",
    "                      serializable_all_results_flat.append(new_record)\n",
    "\n",
    "                  with open(all_results_save_path, 'w') as f: json.dump(serializable_all_results_flat, f, default=default_serializer, indent=4)\n",
    "                  print(f\"\\nCOMBINED detailed results summary saved to JSON: {all_results_save_path}\")\n",
    "             except Exception as e:\n",
    "                  print(f\"\\nError saving COMBINED results summary: {e}\")\n",
    "        else:\n",
    "            print(\"\\nNo results generated across all assets to save.\")\n",
    "\n",
    "    # # Final cleanup of the base temporary model directory\n",
    "    # if os.path.exists(local_temp_models_base):\n",
    "    #     print(f\"\\nCleaning up base temporary model directory: {local_temp_models_base}\")\n",
    "    #     try:\n",
    "    #         shutil.rmtree(local_temp_models_base)\n",
    "    #         print(f\"Successfully removed {local_temp_models_base}\")\n",
    "    #     except OSError as e:\n",
    "    #         print(f\"Error removing {local_temp_models_base}: {e}\")\n",
    "    # else:\n",
    "    #     print(f\"\\nBase temporary model directory {local_temp_models_base} not found, no cleanup needed or already cleaned.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
