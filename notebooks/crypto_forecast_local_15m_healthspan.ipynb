{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a0e95c-10c1-44d5-86d6-7b7e3efad0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FutureWarning for TimeSeriesScorer prediction_length suppressed.\n",
      "Predictor path already exists warning suppressed.\n"
     ]
    }
   ],
   "source": [
    "# @title Import Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines # Needed for custom legend handles\n",
    "import seaborn as sns             # Needed for plotting\n",
    "import os\n",
    "from joblib import Parallel, delayed # <<< ADD FOR PARALLELISM\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from autogluon.common import space\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import display\n",
    "import time\n",
    "import shutil\n",
    "import gc\n",
    "import pyarrow\n",
    "import warnings\n",
    "\n",
    "# Ignore the specific FutureWarning related to TimeSeriesScorer prediction_length\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated.*\", # Match start of message\n",
    "    category=FutureWarning,\n",
    "    module=\"autogluon.timeseries.metrics.abstract\" # Be specific about the source module\n",
    ")\n",
    "print(\"FutureWarning for TimeSeriesScorer prediction_length suppressed.\")\n",
    "\n",
    "# Also ignore the \"path already exists\" warning from Predictor init\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"path already exists!.*\", # Match the start of the warning message\n",
    "    category=UserWarning # This is often a UserWarning, adjust if necessary\n",
    ")\n",
    "print(\"Predictor path already exists warning suppressed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e78ac0-c476-456a-88fd-ad565d1a5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max concurrent runs: 14\n",
      "Base results directory for this run: G:\\My Drive\\code_projects\\btc_forecast\\results\\15m_20250508\n",
      "Looking for datasets in: 'G:\\My Drive\\code_projects\\btc_forecast\\datasets'\n",
      "Script frequency set to: '15m'\n",
      "\n",
      "Selected Models: ['AutoETS']\n",
      "\n",
      "Configuration set and stored in config_params.\n"
     ]
    }
   ],
   "source": [
    "# @title Base Configuration (User Parameters, Paths)\n",
    "\n",
    "# ----- Define LOCAL Paths -----\n",
    "# Use raw strings (r\"...\") for Windows paths\n",
    "local_base_path = r\"G:\\My Drive\\code_projects\\btc_forecast\" # <<< UPDATE TO YOUR LOCAL BASE PATH\n",
    "\n",
    "# --- Directory Paths ---\n",
    "datasets_dir = os.path.join(local_base_path, 'datasets')     # Directory containing .feather files\n",
    "results_base_dir = os.path.join(local_base_path, 'results') # Top-level results directory\n",
    "\n",
    "# --- Parallelism Control ---\n",
    "num_concurrent_runs = 15 # <<< SET MAX NUMBER OF PARALLEL ASSET RUNS\n",
    "print(f\"Setting max concurrent runs: {num_concurrent_runs}\")\n",
    "\n",
    "# --- Frequency & Date for Results ---\n",
    "frequency = '15m' # Explicitly set frequency for filtering files and naming\n",
    "date_today = pd.Timestamp.now().strftime(\"%Y%m%d\") # Use YYYYMMDD format\n",
    "results_dir_timedated = os.path.join(results_base_dir, f\"{frequency}_{date_today}\") # e.g., results/15m_20250506\n",
    "\n",
    "try:\n",
    "    os.makedirs(results_dir_timedated, exist_ok=True)\n",
    "    print(f\"Base results directory for this run: {results_dir_timedated}\")\n",
    "except OSError as e:\n",
    "     print(f\"Warning: Could not create results directory {results_dir_timedated}: {e}\")\n",
    "\n",
    "print(f\"Looking for datasets in: '{datasets_dir}'\")\n",
    "print(f\"Script frequency set to: '{frequency}'\")\n",
    "\n",
    "# --- Column Names ---\n",
    "timestamp_col = 'date'\n",
    "potential_target_cols = ['volume', 'open', 'high', 'low', 'close']\n",
    "all_value_cols = [timestamp_col] + potential_target_cols\n",
    "\n",
    "# --- User-defined Experiment Parameters ---\n",
    "prediction_length = 1\n",
    "max_train_dur = 2 * (60*60) # Reduced for local testing? Adjust as needed.\n",
    "desired_windows = 10\n",
    "backtest_strategy = \"rolling\"\n",
    "error_metric = \"WQL\"\n",
    "desired_quantiles = [0.05, 0.5, 0.95]\n",
    "\n",
    "# --- Model Selection ---\n",
    "full_hp = {\"AutoETS\": {}}\n",
    "selected_algorithms = [\"AutoETS\"]\n",
    "selected_hp = {alg: full_hp[alg] for alg in selected_algorithms if alg in full_hp}\n",
    "print(\"\\nSelected Models:\", selected_algorithms)\n",
    "\n",
    "# === Experiment Setup ===\n",
    "candle_counts_to_test = [100, 250, 500, 750, 1000, 1500, 2000]\n",
    "targets_to_process = potential_target_cols\n",
    "\n",
    "# ----- Healthspan Evaluation Parameters -----\n",
    "enable_healthspan_evaluation = True\n",
    "health_evaluation_horizons = [50, 100, 250, 500, 750, 1000]  # <<< YOUR DEFINED HORIZONS\n",
    "# evaluation_chunk_size = 50 # <<< COMMENT OUT OR REMOVE\n",
    "# max_evaluation_chunks = 20   # <<< COMMENT OUT OR REMOVE\n",
    "healthspan_threshold_std_devs = 1.5\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option('display.precision', 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# --- Store config in a dictionary for passing to parallel function ---\n",
    "config_params = {\n",
    "    \"datasets_dir\": datasets_dir,\n",
    "    \"results_dir_timedated\": results_dir_timedated,\n",
    "    \"frequency\": frequency,\n",
    "    \"date_today\": date_today,\n",
    "    \"timestamp_col\": timestamp_col,\n",
    "    \"potential_target_cols\": potential_target_cols,\n",
    "    \"all_value_cols\": all_value_cols,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"max_train_dur\": max_train_dur,\n",
    "    \"desired_windows\": desired_windows,\n",
    "    \"backtest_strategy\": backtest_strategy,\n",
    "    \"error_metric\": error_metric,\n",
    "    \"desired_quantiles\": desired_quantiles,\n",
    "    \"selected_hp\": selected_hp,\n",
    "    \"candle_counts_to_test\": candle_counts_to_test,\n",
    "    \"targets_to_process\": targets_to_process,\n",
    "    \"enable_healthspan_evaluation\": enable_healthspan_evaluation,\n",
    "    \"health_evaluation_horizons\": health_evaluation_horizons,\n",
    "    \"healthspan_threshold_std_devs\": healthspan_threshold_std_devs,\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration set and stored in config_params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae51e7-fa14-44f5-85d4-2827a72e8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper Functions (Serializer & Healthspan Calc)\n",
    "\n",
    "# --- Define serializer function ---\n",
    "def default_serializer(obj):\n",
    "    if isinstance(obj, (np.integer, np.floating, np.bool_)): return obj.item()\n",
    "    elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    elif isinstance(obj, pd.Timestamp): return obj.isoformat()\n",
    "    elif isinstance(obj, pd.Timedelta):\n",
    "         if pd.isna(obj): return None # Handle NaT\n",
    "         return obj.total_seconds()\n",
    "    elif hasattr(obj, 'get_hyperparameters') :\n",
    "        try: return obj.get_hyperparameters()\n",
    "        except Exception: return str(obj)\n",
    "    elif isinstance(obj, (dict, list, str, int, float, bool, type(None))): return obj\n",
    "    else: return str(obj)\n",
    "\n",
    "\n",
    "def calculate_healthspan(wql_scores, horizons, initial_wql, threshold_std_devs): # MODIFIED SIGNATURE\n",
    "    \"\"\" Calculates healthspan based on when WQL crosses a threshold at specific horizons. \"\"\"\n",
    "    if not wql_scores or not horizons or len(wql_scores) != len(horizons) or initial_wql is None or pd.isna(initial_wql):\n",
    "        # print(\"  [Healthspan Calc] Cannot calculate: Invalid inputs (scores, horizons, or initial_wql).\")\n",
    "        return None, None\n",
    "    \n",
    "    # Assuming wql_scores and horizons passed are already aligned and numeric scores are filtered if needed by caller\n",
    "    valid_scores_list = [float(s) for s in wql_scores if isinstance(s, (int, float, np.number)) and pd.notna(s)]\n",
    "    if not valid_scores_list: # Should ideally be guaranteed by caller\n",
    "        # print(\"  [Healthspan Calc] Cannot calculate: No valid numeric scores found.\")\n",
    "        return None, None\n",
    "\n",
    "    scores_array = np.array(valid_scores_list) # Use only valid scores for std dev\n",
    "    wql_std_dev = 0\n",
    "    if len(scores_array) > 1:\n",
    "        wql_std_dev = np.std(scores_array)\n",
    "    # elif len(scores_array) == 1:\n",
    "        # print(\"  [Healthspan Calc] Warning: Only 1 valid score for std dev calculation...\")\n",
    "\n",
    "    actual_wql_baseline = -initial_wql # Assuming initial_wql is the 'score_val' (negative WQL)\n",
    "    threshold = actual_wql_baseline + abs(threshold_std_devs * wql_std_dev) # WQL degrades if it RISES above this\n",
    "    # print(f\"  [Healthspan Calc] Initial Score (-WQL): {initial_wql:.4f}, Actual WQL Baseline: {actual_wql_baseline:.4f}, Std Dev (WQL): {wql_std_dev:.4f}, Threshold (WQL): {threshold:.4f}\")\n",
    "\n",
    "    healthspan_steps = None\n",
    "    # Iterate through the original wql_scores and their corresponding horizons\n",
    "    for i, score in enumerate(wql_scores): # wql_scores here is the list of WQL values (not -WQL)\n",
    "        if pd.isna(score):\n",
    "            continue\n",
    "        if float(score) > threshold: # If the actual WQL score exceeds the calculated degradation threshold\n",
    "            healthspan_steps = horizons[i] # The healthspan is the horizon at which this occurred\n",
    "            # print(f\"  [Healthspan Calc] Threshold crossed at horizon {horizons[i]} (Score: {float(score):.4f}). Estimated healthspan: {healthspan_steps} steps.\")\n",
    "            break\n",
    "            \n",
    "    if healthspan_steps is None and wql_scores: # If threshold was never crossed\n",
    "        # print(f\"  [Healthspan Calc] Threshold not crossed within evaluated horizons up to {horizons[-1] if horizons else 'N/A'}.\")\n",
    "        healthspan_steps = horizons[-1] if horizons else None # Report the max horizon evaluated as healthspan\n",
    "\n",
    "    return healthspan_steps, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f44e3d4-dadb-45c3-9bc9-e1a85175cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured base temporary model directory exists: ag_temp_models_15m\n"
     ]
    }
   ],
   "source": [
    "# @title File Discovery and Main Processing Loop\n",
    "\n",
    "# --- DEFINE AND CREATE BASE TEMP MODEL DIRECTORY (ACCESSIBLE GLOBALLY WITHIN THIS CELL) ---\n",
    "local_temp_models_base = \"ag_temp_models_\"+frequency # Define it here\n",
    "try:\n",
    "    # Clean up this base temp directory from previous full script runs, if it exists\n",
    "    if os.path.exists(local_temp_models_base):\n",
    "        print(f\"Removing existing base temporary model directory: {local_temp_models_base}\")\n",
    "        shutil.rmtree(local_temp_models_base)\n",
    "    os.makedirs(local_temp_models_base, exist_ok=True)\n",
    "    print(f\"Ensured base temporary model directory exists: {local_temp_models_base}\")\n",
    "except OSError as e:\n",
    "    print(f\"Warning: Could not create/clean base temporary model directory {local_temp_models_base}: {e}\")\n",
    "\n",
    "\n",
    "# --- Asset Processing Function (for Parallelism) ---\n",
    "def process_asset(asset_file_path, config):\n",
    "    \"\"\"\n",
    "    Loads, processes, trains, evaluates, and plots results for a single asset file.\n",
    "    Args:\n",
    "        asset_file_path (str): Path to the .feather asset file.\n",
    "        config (dict): Dictionary containing all configuration parameters.\n",
    "    Returns:\n",
    "        list: A list of result dictionaries for all targets/candle counts processed\n",
    "              for this asset, or an empty list if processing fails early.\n",
    "    \"\"\"\n",
    "    # Extract config parameters\n",
    "    frequency = config[\"frequency\"]\n",
    "    timestamp_col = config[\"timestamp_col\"]\n",
    "    potential_target_cols = config[\"potential_target_cols\"]\n",
    "    all_value_cols = config[\"all_value_cols\"]\n",
    "    prediction_length = config[\"prediction_length\"]\n",
    "    max_train_dur = config[\"max_train_dur\"]\n",
    "    desired_windows = config[\"desired_windows\"]\n",
    "    backtest_strategy = config[\"backtest_strategy\"]\n",
    "    error_metric = config[\"error_metric\"] # Should be \"WQL\" for healthspan logic\n",
    "    desired_quantiles = config[\"desired_quantiles\"]\n",
    "    selected_hp = config[\"selected_hp\"]\n",
    "    candle_counts_to_test = config[\"candle_counts_to_test\"]\n",
    "    global_targets_to_process = config[\"targets_to_process\"]\n",
    "    \n",
    "    # Healthspan related parameters\n",
    "    enable_healthspan_evaluation = config[\"enable_healthspan_evaluation\"]\n",
    "    health_evaluation_horizons = config.get(\"health_evaluation_horizons\", []) # NEW\n",
    "    healthspan_threshold_std_devs = config[\"healthspan_threshold_std_devs\"]\n",
    "    \n",
    "    results_dir_timedated = config[\"results_dir_timedated\"]\n",
    "    date_today = config[\"date_today\"]\n",
    "    # local_temp_models_base is globally accessible from Cell [4]'s scope\n",
    "\n",
    "    asset_filename = os.path.basename(asset_file_path)\n",
    "    asset_results = []\n",
    "\n",
    "    print(f\"--- Starting Processing: {asset_filename} (PID: {os.getpid()}) ---\")\n",
    "\n",
    "    try:\n",
    "        asset_name_part = asset_filename.split(f'-{frequency}-futures')[0]\n",
    "        current_item_id = f\"{asset_name_part}_FUT_{frequency}\"\n",
    "    except Exception as e:\n",
    "        print(f\"[{asset_filename}] Warning: Could not derive asset name. Skipping. Error: {e}\")\n",
    "        return asset_results\n",
    "\n",
    "    print(f\"[{asset_name_part}] Loading asset data...\")\n",
    "    try:\n",
    "        df_full_asset_loaded = pd.read_feather(asset_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{asset_name_part}] Error loading asset file: {e}. Skipping.\")\n",
    "        return asset_results\n",
    "\n",
    "    print(f\"[{asset_name_part}] Cleaning and preprocessing...\")\n",
    "    cols_to_keep_asset = [col for col in all_value_cols if col in df_full_asset_loaded.columns]\n",
    "    df_full_asset_cleaned = df_full_asset_loaded[cols_to_keep_asset].copy()\n",
    "\n",
    "    if timestamp_col not in df_full_asset_cleaned.columns: print(f\"[{asset_name_part}] Error: Timestamp column missing. Skipping.\"); return asset_results\n",
    "    asset_targets_available = [col for col in potential_target_cols if col in df_full_asset_cleaned.columns]\n",
    "    if not asset_targets_available: print(f\"[{asset_name_part}] Error: No target columns found. Skipping.\"); return asset_results\n",
    "\n",
    "    df_full_asset_cleaned[timestamp_col] = pd.to_datetime(df_full_asset_cleaned[timestamp_col], errors='coerce')\n",
    "    numeric_cols_asset = asset_targets_available\n",
    "    for col in numeric_cols_asset: df_full_asset_cleaned[col] = pd.to_numeric(df_full_asset_cleaned[col], errors='coerce')\n",
    "    cols_to_check_na_asset = [timestamp_col] + numeric_cols_asset\n",
    "    df_full_asset_cleaned = df_full_asset_cleaned.dropna(subset=cols_to_check_na_asset).copy()\n",
    "\n",
    "    if not df_full_asset_cleaned.empty:\n",
    "        asset_timestamp_dtype = df_full_asset_cleaned[timestamp_col].dtype\n",
    "        is_tz_aware_asset = pd.api.types.is_datetime64_ns_dtype(asset_timestamp_dtype) and df_full_asset_cleaned[timestamp_col].dt.tz is not None\n",
    "        if is_tz_aware_asset: current_time_compare = pd.Timestamp.now(tz=df_full_asset_cleaned[timestamp_col].dt.tz)\n",
    "        else: current_time_compare = pd.Timestamp.now().tz_localize(None)\n",
    "        df_full_asset_cleaned = df_full_asset_cleaned.sort_values(timestamp_col).reset_index(drop=True)\n",
    "        df_full_asset_cleaned = df_full_asset_cleaned[df_full_asset_cleaned[timestamp_col] < current_time_compare]\n",
    "\n",
    "    if df_full_asset_cleaned.empty: print(f\"[{asset_name_part}] DataFrame empty after filtering. Skipping.\"); return asset_results\n",
    "\n",
    "    print(f\"[{asset_name_part}] Starting Target Loop...\")\n",
    "    asset_loop_targets = [t for t in global_targets_to_process if t in asset_targets_available]\n",
    "    \n",
    "    min_data_for_any_eval_slice = prediction_length + 1 # Min length for any slice passed to predictor.evaluate\n",
    "\n",
    "    for current_target_raw in asset_loop_targets:\n",
    "        print(f\"\\n\\n[{asset_name_part}] {'#'*60}\")\n",
    "        print(f\"[{asset_name_part}] ### Processing Target: {current_target_raw} ###\")\n",
    "        print(f\"[{asset_name_part}] {'#'*60}\\n\")\n",
    "\n",
    "        df_experiment_base = df_full_asset_cleaned.copy()\n",
    "        # results_summary_target_for_asset = [] # This was defined but not used; asset_results is used directly.\n",
    "\n",
    "        ag_target_col_name = current_target_raw\n",
    "        if current_target_raw == 'volume':\n",
    "            ag_target_col_name = f\"{current_target_raw}_log1p\"\n",
    "            print(f\"[{asset_name_part}/{current_target_raw}] Applying log1p transformation. Training target will be: '{ag_target_col_name}'\")\n",
    "            if (df_experiment_base[current_target_raw] < 0).any():\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] Warning: Clipping negative values in '{current_target_raw}' before log1p.\")\n",
    "                df_experiment_base[current_target_raw] = df_experiment_base[current_target_raw].clip(lower=0)\n",
    "            df_experiment_base[ag_target_col_name] = np.log1p(df_experiment_base[current_target_raw])\n",
    "        else:\n",
    "            print(f\"[{asset_name_part}/{current_target_raw}] Using raw column as training target: '{ag_target_col_name}'\")\n",
    "\n",
    "        valid_candle_counts_for_target = []\n",
    "        if enable_healthspan_evaluation and health_evaluation_horizons:\n",
    "            # Max horizon that is valid (long enough for an evaluation slice)\n",
    "            valid_horizons = [h for h in health_evaluation_horizons if h >= min_data_for_any_eval_slice]\n",
    "            if not valid_horizons:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] No valid health evaluation horizons >= {min_data_for_any_eval_slice}. Healthspan eval might be skipped.\")\n",
    "                max_eval_points_needed = 0\n",
    "            else:\n",
    "                max_eval_points_needed = max(valid_horizons)\n",
    "            \n",
    "            # A candle_count 'c' is valid if data has length for 'c' AND 'max_eval_points_needed' after it.\n",
    "            # And 'c' itself must be long enough for training.\n",
    "            valid_candle_counts_for_target = [\n",
    "                c for c in candle_counts_to_test \n",
    "                if c >= (prediction_length + 1) and (c + max_eval_points_needed) <= len(df_experiment_base)\n",
    "            ]\n",
    "            if not valid_candle_counts_for_target and candle_counts_to_test:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] Skipping target: Not enough data for any candle_count with specified healthspan horizons. Need {prediction_length+1} for train and up to {max_eval_points_needed} for eval.\")\n",
    "                continue\n",
    "        else: # Healthspan not enabled or no horizons\n",
    "            max_eval_points_needed = 0 # Ensure this is defined\n",
    "            valid_candle_counts_for_target = [c for c in candle_counts_to_test if c >= (prediction_length + 1)]\n",
    "            if not valid_candle_counts_for_target and candle_counts_to_test:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] Skipping target: Not enough data for training (min {prediction_length+1} candles).\")\n",
    "                continue\n",
    "        \n",
    "        if not valid_candle_counts_for_target: # If still empty after all checks\n",
    "             print(f\"[{asset_name_part}/{current_target_raw}] No valid candle counts to test for this target. Skipping.\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        print(f\"\\n[{asset_name_part}/{current_target_raw}] {'='*10} Starting Candle Count Loop {'='*10}\")\n",
    "        print(f\"[{asset_name_part}/{current_target_raw}] Testing candle_counts: {valid_candle_counts_for_target}\")\n",
    "\n",
    "        for candle_count in valid_candle_counts_for_target:\n",
    "            print(f\"\\n[{asset_name_part}/{current_target_raw}] --- Running Candle Count: {candle_count} ---\")\n",
    "            iteration_start_time = time.time()\n",
    "            fit_successful = False; best_score_val_internal = None; error_message = None; fit_num_val_windows_used = 0\n",
    "            train_data_tsdf = None; predictor = None; external_eval_scores = []; healthspan_steps = None; wql_threshold = None\n",
    "            training_data_end_time = None # Initialize\n",
    "\n",
    "            # Uses global 'local_temp_models_base'\n",
    "            path_suffix = f\"{asset_name_part}_{frequency}_{current_target_raw}_{backtest_strategy}_{candle_count}cndl_{date_today}\" # Added date_today for more uniqueness within a day\n",
    "            models_dir = os.path.join(local_temp_models_base, path_suffix)\n",
    "            try:\n",
    "                if os.path.exists(models_dir): shutil.rmtree(models_dir)\n",
    "                os.makedirs(models_dir, exist_ok=True)\n",
    "            except OSError as e:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Error creating model dir: {e}. Skipping.\")\n",
    "                asset_results.append({'target': current_target_raw,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': f'Local model dir error: {e}','time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': 0,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None}); continue\n",
    "\n",
    "            df_train_filtered = pd.DataFrame()\n",
    "            df_eval_slice = pd.DataFrame()\n",
    "\n",
    "            current_max_eval_points_needed = 0\n",
    "            if enable_healthspan_evaluation and health_evaluation_horizons:\n",
    "                valid_horizons_for_setup = [h for h in health_evaluation_horizons if h >= min_data_for_any_eval_slice]\n",
    "                if valid_horizons_for_setup:\n",
    "                    current_max_eval_points_needed = max(valid_horizons_for_setup)\n",
    "            \n",
    "            if current_max_eval_points_needed > 0 :\n",
    "                # Calculate available points for evaluation after taking 'candle_count' for training\n",
    "                max_possible_eval_points_after_train = max(0, len(df_experiment_base) - candle_count)\n",
    "                actual_eval_points_for_slice = min(current_max_eval_points_needed, max_possible_eval_points_after_train)\n",
    "\n",
    "                if actual_eval_points_for_slice >= min_data_for_any_eval_slice:\n",
    "                    df_eval_slice = df_experiment_base.tail(actual_eval_points_for_slice).copy()\n",
    "                    train_end_idx = len(df_experiment_base) - actual_eval_points_for_slice\n",
    "                    train_start_idx = max(0, train_end_idx - candle_count)\n",
    "                    df_train_filtered = df_experiment_base.iloc[train_start_idx:train_end_idx].copy()\n",
    "                else: # Not enough data for any meaningful healthspan eval for this candle_count\n",
    "                    print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Not enough data ({actual_eval_points_for_slice}) for any specified health horizon. Training on tail data without healthspan eval.\")\n",
    "                    df_train_filtered = df_experiment_base.tail(candle_count).copy()\n",
    "                    # df_eval_slice remains empty\n",
    "            else: # Healthspan not enabled, no horizons, or no valid horizons\n",
    "                df_train_filtered = df_experiment_base.tail(candle_count).copy()\n",
    "                # df_eval_slice remains empty\n",
    "\n",
    "            num_points_in_train = len(df_train_filtered)\n",
    "            if df_train_filtered.empty or num_points_in_train < (prediction_length + 1):\n",
    "                error_msg_detail = 'Empty training slice' if df_train_filtered.empty else f'Insufficient training data ({num_points_in_train}, need {prediction_length+1})'\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Error: {error_msg_detail}. Skipping.\")\n",
    "                asset_results.append({'target': current_target_raw,'asset': asset_name_part,'candle_count': candle_count,'error': error_msg_detail,'best_score_val_internal': None,'time_taken_seconds': time.time()-iteration_start_time,'num_data_points_train':0,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None})\n",
    "                if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True);\n",
    "                continue\n",
    "            \n",
    "            training_data_end_time = df_train_filtered[timestamp_col].iloc[-1] # Set this after df_train_filtered is confirmed non-empty\n",
    "\n",
    "            df_train_filtered[\"item_id\"] = current_item_id\n",
    "            id_column_name = \"item_id\"\n",
    "            if df_train_filtered[timestamp_col].dt.tz is not None:\n",
    "                try:\n",
    "                    df_train_filtered[timestamp_col] = df_train_filtered[timestamp_col].dt.tz_localize(None)\n",
    "                except Exception as tz_e:\n",
    "                    print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] TZ Error: {tz_e}. Skipping.\")\n",
    "                    asset_results.append({'target': current_target_raw,'asset': asset_name_part,'candle_count': candle_count, 'error': f'TZ conversion failed: {tz_e}', 'best_score_val_internal': None, 'time_taken_seconds': time.time()-iteration_start_time, 'num_data_points_train':num_points_in_train, 'num_val_windows_used': 0, 'external_eval_wql_scores': [], 'healthspan_steps': None, 'wql_threshold': None})\n",
    "                    if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True);\n",
    "                    continue\n",
    "            try:\n",
    "                required_cols_tsdf = [id_column_name, timestamp_col, ag_target_col_name]\n",
    "                covariate_cols = [ptc for ptc in potential_target_cols if ptc in df_train_filtered.columns and ptc != current_target_raw]\n",
    "                cols_for_tsdf = list(set(required_cols_tsdf + covariate_cols))\n",
    "\n",
    "                missing_cols = [col for col in required_cols_tsdf if col not in df_train_filtered.columns] # Check against actual df_train_filtered\n",
    "                if missing_cols: raise ValueError(f\"Missing required cols for TSDF: {missing_cols} from {df_train_filtered.columns.tolist()}\")\n",
    "\n",
    "                train_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n",
    "                    df_train_filtered[cols_for_tsdf], # Use cols_for_tsdf which should be present\n",
    "                    id_column=id_column_name, timestamp_column=timestamp_col\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Error creating Training TSDF: {e}\")\n",
    "                error_message = f\"Train TSDF creation failed: {e}.\"\n",
    "                asset_results.append({'target': current_target_raw,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': error_message,'time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': num_points_in_train,'num_val_windows_used': 0,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None})\n",
    "                if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True);\n",
    "                continue\n",
    "\n",
    "            fit_num_val_windows_used = 0; fit_val_step_size = 1\n",
    "            if desired_windows > 0:\n",
    "                total_data_len_val = len(train_data_tsdf); min_train_for_val = prediction_length; \n",
    "                # required_for_val = prediction_length * desired_windows * prediction_length # This seems too large\n",
    "                required_for_val = prediction_length * desired_windows # A window needs prediction_length, and step size matters\n",
    "                if total_data_len_val >= min_train_for_val + required_for_val: \n",
    "                    fit_num_val_windows_used = desired_windows\n",
    "                    fit_val_step_size = prediction_length # Common step size\n",
    "                else: \n",
    "                    max_possible = (total_data_len_val - min_train_for_val) // prediction_length # How many blocks of prediction_length fit\n",
    "                    fit_num_val_windows_used = max(0, min(desired_windows, max_possible))\n",
    "                    if fit_num_val_windows_used > 0: fit_val_step_size = prediction_length\n",
    "            \n",
    "            try:\n",
    "                predictor = TimeSeriesPredictor(prediction_length=prediction_length, target=ag_target_col_name, eval_metric=error_metric, path=models_dir, quantile_levels=desired_quantiles)\n",
    "            except Exception as e:\n",
    "                error_message = f'Predictor init failed: {e}'; print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Predictor Init Error: {e}\");\n",
    "                asset_results.append({'target': current_target_raw,'asset': asset_name_part,'candle_count': candle_count,'best_score_val_internal': None,'error': error_message,'time_taken_seconds': time.time() - iteration_start_time,'num_data_points_train': num_points_in_train,'num_val_windows_used': fit_num_val_windows_used,'external_eval_wql_scores': [],'healthspan_steps': None,'wql_threshold': None})\n",
    "                if os.path.exists(models_dir): shutil.rmtree(models_dir, ignore_errors=True);\n",
    "                continue\n",
    "            \n",
    "            print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Fitting model on target '{ag_target_col_name}' with {fit_num_val_windows_used} val windows...\")\n",
    "            hpo_tune_kwargs = None\n",
    "            try:\n",
    "                predictor.fit(train_data_tsdf, presets=\"best_quality\", time_limit=max_train_dur, num_val_windows=fit_num_val_windows_used, val_step_size=fit_val_step_size, refit_full=True, hyperparameters=selected_hp, hyperparameter_tune_kwargs=hpo_tune_kwargs, enable_ensemble=False, verbosity=0)\n",
    "                fit_successful = True\n",
    "            except Exception as fit_e: error_message = f\"Fitting failed: {fit_e}\"; print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] ERROR during fitting: {fit_e}\")\n",
    "\n",
    "            best_model_name = \"N/A\"; leaderboard_df = None;\n",
    "            if fit_successful:\n",
    "                try:\n",
    "                    if fit_num_val_windows_used > 0: leaderboard_df = predictor.leaderboard(silent=True)\n",
    "                    if leaderboard_df is not None and not leaderboard_df.empty and 'score_val' in leaderboard_df.columns and pd.api.types.is_numeric_dtype(leaderboard_df['score_val']):\n",
    "                        ets_lb = leaderboard_df[leaderboard_df['model'].str.contains(\"ETS\", na=False)] # Check specific model if needed\n",
    "                        if not ets_lb.empty: \n",
    "                            score = ets_lb[\"score_val\"].iloc[0]\n",
    "                            if pd.notna(score): \n",
    "                                best_score_val_internal = score\n",
    "                                best_model_name = ets_lb[\"model\"].iloc[0]\n",
    "                        elif not leaderboard_df.empty: # Fallback to best model if ETS not found but LB exists\n",
    "                             best_score_val_internal = leaderboard_df[\"score_val\"].iloc[0]\n",
    "                             best_model_name = leaderboard_df[\"model\"].iloc[0]\n",
    "                except Exception as lb_e: print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Leaderboard Error: {lb_e}\"); best_score_val_internal = None\n",
    "\n",
    "            # --- Healthspan Evaluation with Horizons ---\n",
    "            if fit_successful and enable_healthspan_evaluation and not df_eval_slice.empty:\n",
    "                external_eval_scores = []\n",
    "                future_tsdf_full = None # Initialize\n",
    "                try:\n",
    "                    future_data_for_eval = df_eval_slice.copy()\n",
    "                    future_data_for_eval[\"item_id\"] = current_item_id\n",
    "                    if future_data_for_eval[timestamp_col].dt.tz is not None:\n",
    "                        future_data_for_eval[timestamp_col] = future_data_for_eval[timestamp_col].dt.tz_localize(None)\n",
    "\n",
    "                    if ag_target_col_name not in future_data_for_eval.columns:\n",
    "                        print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] CRITICAL Error: Training target '{ag_target_col_name}' not in evaluation slice. Skipping external eval.\")\n",
    "                        if error_message is None: error_message = f\"Target '{ag_target_col_name}' missing in eval slice for healthspan.\"\n",
    "                    else:\n",
    "                        eval_required_cols = [id_column_name, timestamp_col, ag_target_col_name]\n",
    "                        eval_covariate_cols = [ptc for ptc in potential_target_cols if ptc in future_data_for_eval.columns and ptc != current_target_raw]\n",
    "                        eval_cols_for_tsdf = list(set(eval_required_cols + eval_covariate_cols))\n",
    "                        \n",
    "                        missing_eval_cols = [col for col in eval_required_cols if col not in future_data_for_eval.columns]\n",
    "                        if missing_eval_cols:\n",
    "                             print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Error: Missing required columns {missing_eval_cols} for eval TSDF. Skipping external eval.\")\n",
    "                             if error_message is None: error_message = \"Missing cols for eval TSDF.\"\n",
    "                        else:\n",
    "                            future_tsdf_full = TimeSeriesDataFrame.from_data_frame(future_data_for_eval[eval_cols_for_tsdf], id_column=id_column_name, timestamp_column=timestamp_col)\n",
    "                    \n",
    "                    if future_tsdf_full is not None and not future_tsdf_full.empty:\n",
    "                        feasible_horizons = sorted(list(set(\n",
    "                            h for h in health_evaluation_horizons\n",
    "                            if h <= len(future_tsdf_full) and h >= min_data_for_any_eval_slice\n",
    "                        )))\n",
    "\n",
    "                        if not feasible_horizons:\n",
    "                            print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] No feasible health evaluation horizons with available data ({len(future_tsdf_full)} points).\")\n",
    "                        else:\n",
    "                            print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Evaluating at feasible horizons: {feasible_horizons}\")\n",
    "\n",
    "                        for horizon_length in feasible_horizons:\n",
    "                            current_evaluation_data_slice = future_tsdf_full.iloc[0:horizon_length]\n",
    "                            time_diff_eval = pd.NaT\n",
    "                            try:\n",
    "                                horizon_end_timestamp_obj = current_evaluation_data_slice.index[-1][1]\n",
    "                                horizon_end_time_eval = pd.to_datetime(horizon_end_timestamp_obj)\n",
    "                                training_data_end_time_ts = pd.to_datetime(training_data_end_time)\n",
    "                                if pd.notna(horizon_end_time_eval) and pd.notna(training_data_end_time_ts): time_diff_eval = horizon_end_time_eval - training_data_end_time_ts\n",
    "                            except Exception: time_diff_eval = pd.NaT\n",
    "                            \n",
    "                            try:\n",
    "                                eval_metrics = predictor.evaluate(current_evaluation_data_slice, model=best_model_name, metrics=[error_metric]) # Use best_model_name or 'AutoETS_FULL' if always ETS\n",
    "                                wql_score_chunk = eval_metrics.get(error_metric, np.nan)\n",
    "                                external_eval_scores.append({'time_since_train': time_diff_eval, 'WQL': wql_score_chunk, 'horizon': horizon_length})\n",
    "                            except Exception as eval_e:\n",
    "                                print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] ERROR evaluating horizon {horizon_length}: {eval_e}\")\n",
    "                                external_eval_scores.append({'time_since_train': time_diff_eval, 'WQL': np.nan, 'horizon': horizon_length})\n",
    "                except Exception as ext_eval_setup_e:\n",
    "                    print(f\"[{asset_name_part}/{current_target_raw}/{candle_count}] Healthspan evaluation setup error: {ext_eval_setup_e}\")\n",
    "                    if error_message is None: error_message = f\"Healthspan eval setup failed: {ext_eval_setup_e}\"\n",
    "\n",
    "                if external_eval_scores:\n",
    "                    valid_eval_items = [item for item in external_eval_scores if pd.notna(item['WQL']) and item.get('horizon') is not None] # Ensure horizon is present\n",
    "                    wql_values_for_calc = [item['WQL'] for item in valid_eval_items]\n",
    "                    horizons_for_calc = [item['horizon'] for item in valid_eval_items]\n",
    "\n",
    "                    if wql_values_for_calc and horizons_for_calc and best_score_val_internal is not None and pd.notna(best_score_val_internal):\n",
    "                        baseline_wql_score_for_healthspan = best_score_val_internal # This is already -WQL\n",
    "                        healthspan_steps, wql_threshold = calculate_healthspan(\n",
    "                            wql_scores=wql_values_for_calc,\n",
    "                            horizons=horizons_for_calc,\n",
    "                            initial_wql=baseline_wql_score_for_healthspan, # Pass the -WQL score\n",
    "                            threshold_std_devs=healthspan_threshold_std_devs\n",
    "                        )\n",
    "            # --- End Healthspan Evaluation ---\n",
    "\n",
    "            iteration_end_time = time.time()\n",
    "            asset_results.append({\n",
    "                'target': current_target_raw, 'asset': asset_name_part, 'candle_count': candle_count,\n",
    "                'best_score_val_internal': best_score_val_internal, 'error': error_message,\n",
    "                'time_taken_seconds': iteration_end_time - iteration_start_time,\n",
    "                'num_data_points_train': num_points_in_train, 'num_val_windows_used': fit_num_val_windows_used,\n",
    "                'external_eval_wql_scores': external_eval_scores, # Contains 'horizon'\n",
    "                'healthspan_steps': healthspan_steps,\n",
    "                'wql_threshold': wql_threshold,\n",
    "                'best_model_name': best_model_name\n",
    "            })\n",
    "            print(f\"--- Finished Iteration: Asset={asset_name_part}, Target={current_target_raw}, Candles={candle_count}, Time={asset_results[-1]['time_taken_seconds']:.1f}s, Score={best_score_val_internal if best_score_val_internal is not None else 'N/A'}, Healthspan={healthspan_steps if healthspan_steps is not None else 'N/A'} ---\")\n",
    "\n",
    "            if predictor is not None: del predictor\n",
    "            if train_data_tsdf is not None: del train_data_tsdf\n",
    "            if leaderboard_df is not None: del leaderboard_df\n",
    "            if 'future_data_for_eval' in locals(): del future_data_for_eval # Explicitly delete\n",
    "            if 'future_tsdf_full' in locals() and future_tsdf_full is not None: del future_tsdf_full\n",
    "            if 'current_evaluation_data_slice' in locals(): del current_evaluation_data_slice\n",
    "            # Aggressively clean the specific models_dir for this iteration if desired by user (they said they'll handle it)\n",
    "            # However, for robustness if running this function standalone:\n",
    "            if os.path.exists(models_dir):\n",
    "                 try: shutil.rmtree(models_dir); # print(f\"Cleaned up {models_dir}\")\n",
    "                 except Exception as e_rm: print(f\"Could not clean up {models_dir}: {e_rm}\")\n",
    "            gc.collect()\n",
    "        # === END CANDLE COUNT LOOP ===\n",
    "        print(f\"\\n[{asset_name_part}/{current_target_raw}] Finished Candle Count Loop.\")\n",
    "\n",
    "        current_target_plot_results = [res for res in asset_results if res['target'] == current_target_raw and res['asset'] == asset_name_part]\n",
    "        results_df_target_plot = pd.DataFrame(current_target_plot_results)\n",
    "\n",
    "        if not results_df_target_plot.empty:\n",
    "            summary_filename = f'{date_today}_{asset_name_part}_{frequency}_{current_target_raw}_{backtest_strategy}_healthspan_summary.json'\n",
    "            summary_save_path = os.path.join(results_dir_timedated, summary_filename)\n",
    "            try:\n",
    "                serializable_current_target_results = []\n",
    "                for record in current_target_plot_results:\n",
    "                    new_record = record.copy(); temp_scores_list = []\n",
    "                    if 'external_eval_wql_scores' in new_record and new_record['external_eval_wql_scores']:\n",
    "                        for score_item in new_record['external_eval_wql_scores']:\n",
    "                            new_score_item = score_item.copy()\n",
    "                            time_delta = new_score_item.get('time_since_train')\n",
    "                            if isinstance(time_delta, pd.Timedelta): new_score_item['time_since_train'] = None if pd.isna(time_delta) else time_delta.total_seconds()\n",
    "                            temp_scores_list.append(new_score_item)\n",
    "                        new_record['external_eval_wql_scores'] = temp_scores_list\n",
    "                    serializable_current_target_results.append(new_record)\n",
    "                with open(summary_save_path, 'w') as f: json.dump(serializable_current_target_results, f, default=default_serializer, indent=4)\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] Detailed summary saved: {summary_save_path}\")\n",
    "            except Exception as e: print(f\"[{asset_name_part}/{current_target_raw}] Error saving JSON summary: {e}\")\n",
    "\n",
    "            plot_data_list = []\n",
    "            for idx_plot, row_plot in results_df_target_plot.iterrows():\n",
    "                cc_plot = row_plot['candle_count']\n",
    "                oos_scores_plot = row_plot.get('external_eval_wql_scores', [])\n",
    "                if oos_scores_plot:\n",
    "                    for score_item_plot in oos_scores_plot:\n",
    "                        wql_plot_val = score_item_plot.get('WQL')\n",
    "                        steps_plot_val = score_item_plot.get('horizon') # Use 'horizon'\n",
    "                        if steps_plot_val is not None and pd.notna(wql_plot_val):\n",
    "                            plot_data_list.append({'candle_count': cc_plot, 'steps_since_train': steps_plot_val, 'WQL': wql_plot_val, 'WQL_neg': -wql_plot_val if pd.notna(wql_plot_val) else np.nan})\n",
    "            plot_df_target_external = pd.DataFrame(plot_data_list) if plot_data_list else pd.DataFrame()\n",
    "            if not plot_df_target_external.empty: plot_df_target_external.dropna(subset=['steps_since_train', 'WQL_neg'], inplace=True)\n",
    "\n",
    "            results_df_filtered_internal = results_df_target_plot.dropna(subset=['best_score_val_internal']).copy()\n",
    "            if not results_df_filtered_internal.empty:\n",
    "                plt.figure(figsize=(12, 6)); plt.plot(results_df_filtered_internal['candle_count'], results_df_filtered_internal['best_score_val_internal'], marker='o', linestyle='-')\n",
    "                plt.title(f'Internal Score (-WQL) vs. Candle Count (Asset: {asset_name_part}, Target: {current_target_raw})'); plt.xlabel('Number of Candles (`candle_count`)'); plt.ylabel(f'Internal Fit Score ({error_metric}, Higher is Better)')\n",
    "                plt.grid(True); plt.xticks(rotation=45); plt.tight_layout()\n",
    "                plot_filename_internal = f'{date_today}_{asset_name_part}_{frequency}_{current_target_raw}_{backtest_strategy}_internal_score_plot.png'\n",
    "                plot_save_path_internal = os.path.join(results_dir_timedated, plot_filename_internal)\n",
    "                try: plt.savefig(plot_save_path_internal, dpi=150); print(f\"[{asset_name_part}/{current_target_raw}] Internal score plot saved: {plot_save_path_internal}\")\n",
    "                except Exception as e: print(f\"[{asset_name_part}/{current_target_raw}] Error saving internal plot: {e}\")\n",
    "                plt.close()\n",
    "\n",
    "            if not plot_df_target_external.empty:\n",
    "                print(f\"[{asset_name_part}/{current_target_raw}] Generating Enhanced Healthspan Plot...\")\n",
    "                plt.style.use('seaborn-v0_8-whitegrid'); fig, ax = plt.subplots(figsize=(16, 9))\n",
    "                candle_counts_plot = sorted(plot_df_target_external['candle_count'].unique())\n",
    "                palette = sns.color_palette(\"viridis\", n_colors=len(candle_counts_plot)); color_map = dict(zip(candle_counts_plot, palette))\n",
    "                sns.lineplot(data=plot_df_target_external, x='steps_since_train', y='WQL_neg', hue='candle_count', hue_order=candle_counts_plot, palette=color_map, marker='.', linewidth=1.5, legend=False, ax=ax)\n",
    "                \n",
    "                is_legend_handles = [];\n",
    "                for cc_plot_iter in candle_counts_plot:\n",
    "                    is_score_row = results_df_target_plot[results_df_target_plot['candle_count'] == cc_plot_iter] # Use full results_df_target_plot\n",
    "                    if not is_score_row.empty:\n",
    "                        is_score = is_score_row['best_score_val_internal'].iloc[0]\n",
    "                        if pd.notna(is_score): \n",
    "                            ax.axhline(y=is_score, color=color_map.get(cc_plot_iter, 'grey'), linestyle=':', linewidth=1.5, alpha=0.9)\n",
    "                            is_legend_handles.append(mlines.Line2D([], [], color=color_map.get(cc_plot_iter, 'grey'), linestyle=':', linewidth=1.5, label=f'CC {cc_plot_iter} (IS Ref: {is_score:.4f})'))\n",
    "                \n",
    "                median_wql_neg = plot_df_target_external.groupby('steps_since_train')['WQL_neg'].median(); median_handle = []\n",
    "                if not median_wql_neg.empty: \n",
    "                    ax.plot(median_wql_neg.index, median_wql_neg.values, color='black', linestyle='--', linewidth=2.5, label='Median OOS')\n",
    "                    median_handle = [mlines.Line2D([], [], color='black', linestyle='--', linewidth=2.5, label='Median OOS')]\n",
    "                \n",
    "                threshold_handle = []; \n",
    "                # Get wql_threshold per candle_count, then average, or take the one from the best CC?\n",
    "                # For now, let's use the mean of available thresholds if they vary by CC.\n",
    "                avg_actual_wql_threshold = results_df_target_plot['wql_threshold'].mean() \n",
    "                if pd.notna(avg_actual_wql_threshold): \n",
    "                    avg_neg_threshold_plot = -avg_actual_wql_threshold # Since WQL_neg is plotted\n",
    "                    ax.axhline(y=avg_neg_threshold_plot, color='red', linestyle='-.', linewidth=2.0, label=f'Avg Degrad. Threshold (Score < {avg_neg_threshold_plot:.4f})')\n",
    "                    threshold_handle = [mlines.Line2D([], [], color='red', linestyle='-.', linewidth=2.0, label=f'Avg Degrad. Threshold (Score < {avg_neg_threshold_plot:.4f})')]\n",
    "                \n",
    "                oos_legend_handles = [mlines.Line2D([], [], color=color_map.get(cc_plot_iter_oos, 'grey'), marker='.', linewidth=1.5, linestyle='-', label=f'CC {cc_plot_iter_oos} (OOS)') for cc_plot_iter_oos in candle_counts_plot]\n",
    "                all_handles = oos_legend_handles + is_legend_handles + median_handle + threshold_handle\n",
    "                valid_handles = [h for h in all_handles if h.get_label() and not h.get_label().startswith('_')]; \n",
    "                ax.legend(handles=valid_handles, title=f'Performance (-{error_metric}, Higher is Better)', loc='best', fontsize='small')\n",
    "                ax.set_title(f\"Model Healthspan: OOS vs IS (Asset: {asset_name_part}, Target: {current_target_raw})\", fontsize=16)\n",
    "                ax.set_xlabel(f\"Evaluation Horizon (Steps Since Training)\", fontsize=12) # UPDATED LABEL\n",
    "                ax.set_ylabel(f\"Performance Score (-{error_metric}, Higher is Better)\", fontsize=12)\n",
    "                ax.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey'); ax.grid(True, which='minor', linestyle=':', linewidth='0.5', color='lightgrey'); ax.minorticks_on()\n",
    "                try:\n",
    "                    plot_filename = f'{date_today}_{asset_name_part}_{frequency}_{current_target_raw}_{backtest_strategy}_healthspan_plot_v2.png'\n",
    "                    plot_save_path = os.path.join(results_dir_timedated, plot_filename)\n",
    "                    plt.savefig(plot_save_path, bbox_inches='tight', dpi=150)\n",
    "                    print(f\"[{asset_name_part}/{current_target_raw}] Enhanced Healthspan plot saved: {plot_save_path}\")\n",
    "                except Exception as e: print(f\"[{asset_name_part}/{current_target_raw}] Error saving plot: {e}\")\n",
    "                plt.close(fig)\n",
    "\n",
    "        # Cleanup (Target specific DFs)\n",
    "        if 'df_experiment_base' in locals(): del df_experiment_base\n",
    "        if 'results_df_target_plot' in locals(): del results_df_target_plot\n",
    "        if 'results_df_filtered_internal' in locals(): del results_df_filtered_internal\n",
    "        if 'plot_df_target_external' in locals(): del plot_df_target_external\n",
    "        gc.collect()\n",
    "\n",
    "    # --- END TARGET LOOP (for current asset) ---\n",
    "    print(f\"\\n[{asset_name_part}] Finished Target Loop.\")\n",
    "    return asset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261a5dd-48b7-4a9f-a09c-860835bbc46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 512 asset files to process:\n",
      " - 1INCH_USDT_USDT-15m-futures.feather\n",
      " - 1000APU_USDT_USDT-15m-futures.feather\n",
      " - 1000BONK_USDT_USDT-15m-futures.feather\n",
      " - 1000BTT_USDT_USDT-15m-futures.feather\n",
      " - 1000CAT_USDT_USDT-15m-futures.feather\n",
      " - 1000CATS_USDT_USDT-15m-futures.feather\n",
      " - 1000FLOKI_USDT_USDT-15m-futures.feather\n",
      " - 1000LUNC_USDT_USDT-15m-futures.feather\n",
      " - 1000MUMU_USDT_USDT-15m-futures.feather\n",
      " - 1000NEIROCTO_USDT_USDT-15m-futures.feather\n",
      " - 1000PEPE_USDT_USDT-15m-futures.feather\n",
      " - 1000RATS_USDT_USDT-15m-futures.feather\n",
      " - 1000TOSHI_USDT_USDT-15m-futures.feather\n",
      " - 1000TURBO_USDT_USDT-15m-futures.feather\n",
      " - 1000X_USDT_USDT-15m-futures.feather\n",
      " - 1000XEC_USDT_USDT-15m-futures.feather\n",
      " - 10000COQ_USDT_USDT-15m-futures.feather\n",
      " - 10000ELON_USDT_USDT-15m-futures.feather\n",
      " - 10000LADYS_USDT_USDT-15m-futures.feather\n",
      " - 10000QUBIC_USDT_USDT-15m-futures.feather\n",
      " - 10000SATS_USDT_USDT-15m-futures.feather\n",
      " - 10000WEN_USDT_USDT-15m-futures.feather\n",
      " - 10000WHY_USDT_USDT-15m-futures.feather\n",
      " - 1000000BABYDOGE_USDT_USDT-15m-futures.feather\n",
      " - 1000000CHEEMS_USDT_USDT-15m-futures.feather\n",
      " - 1000000MOG_USDT_USDT-15m-futures.feather\n",
      " - 1000000PEIPEI_USDT_USDT-15m-futures.feather\n",
      " - A8_USDT_USDT-15m-futures.feather\n",
      " - AAVE_USDT_USDT-15m-futures.feather\n",
      " - ACE_USDT_USDT-15m-futures.feather\n",
      " - ACH_USDT_USDT-15m-futures.feather\n",
      " - ACT_USDT_USDT-15m-futures.feather\n",
      " - ACX_USDT_USDT-15m-futures.feather\n",
      " - ADA_USDT_USDT-15m-futures.feather\n",
      " - AERGO_USDT_USDT-15m-futures.feather\n",
      " - AERO_USDT_USDT-15m-futures.feather\n",
      " - AEVO_USDT_USDT-15m-futures.feather\n",
      " - AGI_USDT_USDT-15m-futures.feather\n",
      " - AGLD_USDT_USDT-15m-futures.feather\n",
      " - AI_USDT_USDT-15m-futures.feather\n",
      " - AI16Z_USDT_USDT-15m-futures.feather\n",
      " - AIOZ_USDT_USDT-15m-futures.feather\n",
      " - AIXBT_USDT_USDT-15m-futures.feather\n",
      " - AKT_USDT_USDT-15m-futures.feather\n",
      " - ALCH_USDT_USDT-15m-futures.feather\n",
      " - ALEO_USDT_USDT-15m-futures.feather\n",
      " - ALGO_USDT_USDT-15m-futures.feather\n",
      " - ALICE_USDT_USDT-15m-futures.feather\n",
      " - ALPACA_USDT_USDT-15m-futures.feather\n",
      " - ALPHA_USDT_USDT-15m-futures.feather\n",
      " - ALT_USDT_USDT-15m-futures.feather\n",
      " - ALU_USDT_USDT-15m-futures.feather\n",
      " - ANIME_USDT_USDT-15m-futures.feather\n",
      " - ANKR_USDT_USDT-15m-futures.feather\n",
      " - APE_USDT_USDT-15m-futures.feather\n",
      " - API3_USDT_USDT-15m-futures.feather\n",
      " - APT_USDT_USDT-15m-futures.feather\n",
      " - AR_USDT_USDT-15m-futures.feather\n",
      " - ARB_USDT_USDT-15m-futures.feather\n",
      " - ARC_USDT_USDT-15m-futures.feather\n",
      " - ARK_USDT_USDT-15m-futures.feather\n",
      " - ARKM_USDT_USDT-15m-futures.feather\n",
      " - ARPA_USDT_USDT-15m-futures.feather\n",
      " - ASTR_USDT_USDT-15m-futures.feather\n",
      " - ATA_USDT_USDT-15m-futures.feather\n",
      " - ATH_USDT_USDT-15m-futures.feather\n",
      " - ATOM_USDT_USDT-15m-futures.feather\n",
      " - AUCTION_USDT_USDT-15m-futures.feather\n",
      " - AUDIO_USDT_USDT-15m-futures.feather\n",
      " - AVA_USDT_USDT-15m-futures.feather\n",
      " - AVAAI_USDT_USDT-15m-futures.feather\n",
      " - AVAIL_USDT_USDT-15m-futures.feather\n",
      " - AVAX_USDT_USDT-15m-futures.feather\n",
      " - AVL_USDT_USDT-15m-futures.feather\n",
      " - AXL_USDT_USDT-15m-futures.feather\n",
      " - AXS_USDT_USDT-15m-futures.feather\n",
      " - B3_USDT_USDT-15m-futures.feather\n",
      " - BABY_USDT_USDT-15m-futures.feather\n",
      " - BADGER_USDT_USDT-15m-futures.feather\n",
      " - BAKE_USDT_USDT-15m-futures.feather\n",
      " - BAL_USDT_USDT-15m-futures.feather\n",
      " - BAN_USDT_USDT-15m-futures.feather\n",
      " - BANANA_USDT_USDT-15m-futures.feather\n",
      " - BANANAS31_USDT_USDT-15m-futures.feather\n",
      " - BAND_USDT_USDT-15m-futures.feather\n",
      " - BAT_USDT_USDT-15m-futures.feather\n",
      " - BB_USDT_USDT-15m-futures.feather\n",
      " - BCH_USDT_USDT-15m-futures.feather\n",
      " - BEAM_USDT_USDT-15m-futures.feather\n",
      " - BEL_USDT_USDT-15m-futures.feather\n",
      " - BERA_USDT_USDT-15m-futures.feather\n",
      " - BICO_USDT_USDT-15m-futures.feather\n",
      " - BIGTIME_USDT_USDT-15m-futures.feather\n",
      " - BIO_USDT_USDT-15m-futures.feather\n",
      " - BLAST_USDT_USDT-15m-futures.feather\n",
      " - BLUR_USDT_USDT-15m-futures.feather\n",
      " - BMT_USDT_USDT-15m-futures.feather\n",
      " - BNB_USDT_USDT-15m-futures.feather\n",
      " - BNT_USDT_USDT-15m-futures.feather\n",
      " - BOBA_USDT_USDT-15m-futures.feather\n",
      " - BOME_USDT_USDT-15m-futures.feather\n",
      " - BR_USDT_USDT-15m-futures.feather\n",
      " - BRETT_USDT_USDT-15m-futures.feather\n",
      " - BROCCOLI_USDT_USDT-15m-futures.feather\n",
      " - BSV_USDT_USDT-15m-futures.feather\n",
      " - BSW_USDT_USDT-15m-futures.feather\n",
      " - BTC_USDT_USDT-15m-futures.feather\n",
      " - C98_USDT_USDT-15m-futures.feather\n",
      " - CAKE_USDT_USDT-15m-futures.feather\n",
      " - CARV_USDT_USDT-15m-futures.feather\n",
      " - CATI_USDT_USDT-15m-futures.feather\n",
      " - CELO_USDT_USDT-15m-futures.feather\n",
      " - CELR_USDT_USDT-15m-futures.feather\n",
      " - CETUS_USDT_USDT-15m-futures.feather\n",
      " - CFX_USDT_USDT-15m-futures.feather\n",
      " - CGPT_USDT_USDT-15m-futures.feather\n",
      " - CHESS_USDT_USDT-15m-futures.feather\n",
      " - CHILLGUY_USDT_USDT-15m-futures.feather\n",
      " - CHR_USDT_USDT-15m-futures.feather\n",
      " - CHZ_USDT_USDT-15m-futures.feather\n",
      " - CKB_USDT_USDT-15m-futures.feather\n",
      " - CLANKER_USDT_USDT-15m-futures.feather\n",
      " - CLOUD_USDT_USDT-15m-futures.feather\n",
      " - COMP_USDT_USDT-15m-futures.feather\n",
      " - COOK_USDT_USDT-15m-futures.feather\n",
      " - COOKIE_USDT_USDT-15m-futures.feather\n",
      " - CORE_USDT_USDT-15m-futures.feather\n",
      " - COS_USDT_USDT-15m-futures.feather\n",
      " - COTI_USDT_USDT-15m-futures.feather\n",
      " - COW_USDT_USDT-15m-futures.feather\n",
      " - CPOOL_USDT_USDT-15m-futures.feather\n",
      " - CRO_USDT_USDT-15m-futures.feather\n",
      " - CRV_USDT_USDT-15m-futures.feather\n",
      " - CTC_USDT_USDT-15m-futures.feather\n",
      " - CTK_USDT_USDT-15m-futures.feather\n",
      " - CTSI_USDT_USDT-15m-futures.feather\n",
      " - CVC_USDT_USDT-15m-futures.feather\n",
      " - CVX_USDT_USDT-15m-futures.feather\n",
      " - CYBER_USDT_USDT-15m-futures.feather\n",
      " - DARK_USDT_USDT-15m-futures.feather\n",
      " - DASH_USDT_USDT-15m-futures.feather\n",
      " - DATA_USDT_USDT-15m-futures.feather\n",
      " - DBR_USDT_USDT-15m-futures.feather\n",
      " - DEEP_USDT_USDT-15m-futures.feather\n",
      " - DEGEN_USDT_USDT-15m-futures.feather\n",
      " - DENT_USDT_USDT-15m-futures.feather\n",
      " - DEXE_USDT_USDT-15m-futures.feather\n",
      " - DGB_USDT_USDT-15m-futures.feather\n",
      " - DODO_USDT_USDT-15m-futures.feather\n",
      " - DOG_USDT_USDT-15m-futures.feather\n",
      " - DOGE_USDT_USDT-15m-futures.feather\n",
      " - DOGS_USDT_USDT-15m-futures.feather\n",
      " - DOT_USDT_USDT-15m-futures.feather\n",
      " - DRIFT_USDT_USDT-15m-futures.feather\n",
      " - DUCK_USDT_USDT-15m-futures.feather\n",
      " - DUSK_USDT_USDT-15m-futures.feather\n",
      " - DYDX_USDT_USDT-15m-futures.feather\n",
      " - DYM_USDT_USDT-15m-futures.feather\n",
      " - EDU_USDT_USDT-15m-futures.feather\n",
      " - EGLD_USDT_USDT-15m-futures.feather\n",
      " - EIGEN_USDT_USDT-15m-futures.feather\n",
      " - ELX_USDT_USDT-15m-futures.feather\n",
      " - ENA_USDT_USDT-15m-futures.feather\n",
      " - ENJ_USDT_USDT-15m-futures.feather\n",
      " - ENS_USDT_USDT-15m-futures.feather\n",
      " - EOS_USDT_USDT-15m-futures.feather\n",
      " - EPIC_USDT_USDT-15m-futures.feather\n",
      " - EPT_USDT_USDT-15m-futures.feather\n",
      " - ETC_USDT_USDT-15m-futures.feather\n",
      " - ETH_USDT_USDT-15m-futures.feather\n",
      " - ETHBTC_USDT_USDT-15m-futures.feather\n",
      " - ETHFI_USDT_USDT-15m-futures.feather\n",
      " - ETHW_USDT_USDT-15m-futures.feather\n",
      " - F_USDT_USDT-15m-futures.feather\n",
      " - FARTCOIN_USDT_USDT-15m-futures.feather\n",
      " - FB_USDT_USDT-15m-futures.feather\n",
      " - FHE_USDT_USDT-15m-futures.feather\n",
      " - FIDA_USDT_USDT-15m-futures.feather\n",
      " - FIL_USDT_USDT-15m-futures.feather\n",
      " - FIO_USDT_USDT-15m-futures.feather\n",
      " - FLM_USDT_USDT-15m-futures.feather\n",
      " - FLOCK_USDT_USDT-15m-futures.feather\n",
      " - FLOW_USDT_USDT-15m-futures.feather\n",
      " - FLR_USDT_USDT-15m-futures.feather\n",
      " - FLUX_USDT_USDT-15m-futures.feather\n",
      " - FORM_USDT_USDT-15m-futures.feather\n",
      " - FORTH_USDT_USDT-15m-futures.feather\n",
      " - FOXY_USDT_USDT-15m-futures.feather\n",
      " - FTN_USDT_USDT-15m-futures.feather\n",
      " - FUEL_USDT_USDT-15m-futures.feather\n",
      " - FWOG_USDT_USDT-15m-futures.feather\n",
      " - FXS_USDT_USDT-15m-futures.feather\n",
      " - G_USDT_USDT-15m-futures.feather\n",
      " - GALA_USDT_USDT-15m-futures.feather\n",
      " - GAS_USDT_USDT-15m-futures.feather\n",
      " - GIGA_USDT_USDT-15m-futures.feather\n",
      " - GLM_USDT_USDT-15m-futures.feather\n",
      " - GLMR_USDT_USDT-15m-futures.feather\n",
      " - GMT_USDT_USDT-15m-futures.feather\n",
      " - GMX_USDT_USDT-15m-futures.feather\n",
      " - GNO_USDT_USDT-15m-futures.feather\n",
      " - GOAT_USDT_USDT-15m-futures.feather\n",
      " - GODS_USDT_USDT-15m-futures.feather\n",
      " - GOMINING_USDT_USDT-15m-futures.feather\n",
      " - GORK_USDT_USDT-15m-futures.feather\n",
      " - GPS_USDT_USDT-15m-futures.feather\n",
      " - GRASS_USDT_USDT-15m-futures.feather\n",
      " - GRIFFAIN_USDT_USDT-15m-futures.feather\n",
      " - GRT_USDT_USDT-15m-futures.feather\n",
      " - GTC_USDT_USDT-15m-futures.feather\n",
      " - GUN_USDT_USDT-15m-futures.feather\n",
      " - HAEDAL_USDT_USDT-15m-futures.feather\n",
      " - HBAR_USDT_USDT-15m-futures.feather\n",
      " - HEI_USDT_USDT-15m-futures.feather\n",
      " - HFT_USDT_USDT-15m-futures.feather\n",
      " - HIFI_USDT_USDT-15m-futures.feather\n",
      " - HIGH_USDT_USDT-15m-futures.feather\n",
      " - HIPPO_USDT_USDT-15m-futures.feather\n",
      " - HIVE_USDT_USDT-15m-futures.feather\n",
      " - HMSTR_USDT_USDT-15m-futures.feather\n",
      " - HNT_USDT_USDT-15m-futures.feather\n",
      " - HOOK_USDT_USDT-15m-futures.feather\n",
      " - HOT_USDT_USDT-15m-futures.feather\n",
      " - HPOS10I_USDT_USDT-15m-futures.feather\n",
      " - HYPE_USDT_USDT-15m-futures.feather\n",
      " - HYPER_USDT_USDT-15m-futures.feather\n",
      " - ICP_USDT_USDT-15m-futures.feather\n",
      " - ICX_USDT_USDT-15m-futures.feather\n",
      " - ID_USDT_USDT-15m-futures.feather\n",
      " - IDEX_USDT_USDT-15m-futures.feather\n",
      " - ILV_USDT_USDT-15m-futures.feather\n",
      " - IMX_USDT_USDT-15m-futures.feather\n",
      " - INIT_USDT_USDT-15m-futures.feather\n",
      " - INJ_USDT_USDT-15m-futures.feather\n",
      " - IO_USDT_USDT-15m-futures.feather\n",
      " - IOST_USDT_USDT-15m-futures.feather\n",
      " - IOTA_USDT_USDT-15m-futures.feather\n",
      " - IOTX_USDT_USDT-15m-futures.feather\n",
      " - IP_USDT_USDT-15m-futures.feather\n",
      " - J_USDT_USDT-15m-futures.feather\n",
      " - JASMY_USDT_USDT-15m-futures.feather\n",
      " - JELLYJELLY_USDT_USDT-15m-futures.feather\n",
      " - JOE_USDT_USDT-15m-futures.feather\n",
      " - JST_USDT_USDT-15m-futures.feather\n",
      " - JTO_USDT_USDT-15m-futures.feather\n",
      " - JUP_USDT_USDT-15m-futures.feather\n",
      " - KAIA_USDT_USDT-15m-futures.feather\n",
      " - KAITO_USDT_USDT-15m-futures.feather\n",
      " - KAS_USDT_USDT-15m-futures.feather\n",
      " - KAVA_USDT_USDT-15m-futures.feather\n",
      " - KDA_USDT_USDT-15m-futures.feather\n",
      " - KERNEL_USDT_USDT-15m-futures.feather\n",
      " - KMNO_USDT_USDT-15m-futures.feather\n",
      " - KNC_USDT_USDT-15m-futures.feather\n",
      " - KOMA_USDT_USDT-15m-futures.feather\n",
      " - KSM_USDT_USDT-15m-futures.feather\n",
      " - L3_USDT_USDT-15m-futures.feather\n",
      " - LDO_USDT_USDT-15m-futures.feather\n",
      " - LEVER_USDT_USDT-15m-futures.feather\n",
      " - LINK_USDT_USDT-15m-futures.feather\n",
      " - LISTA_USDT_USDT-15m-futures.feather\n",
      " - LOOKS_USDT_USDT-15m-futures.feather\n",
      " - LPT_USDT_USDT-15m-futures.feather\n",
      " - LQTY_USDT_USDT-15m-futures.feather\n",
      " - LRC_USDT_USDT-15m-futures.feather\n",
      " - LSK_USDT_USDT-15m-futures.feather\n",
      " - LTC_USDT_USDT-15m-futures.feather\n",
      " - LUMIA_USDT_USDT-15m-futures.feather\n",
      " - LUNA2_USDT_USDT-15m-futures.feather\n",
      " - MAGIC_USDT_USDT-15m-futures.feather\n",
      " - MAJOR_USDT_USDT-15m-futures.feather\n",
      " - MANA_USDT_USDT-15m-futures.feather\n",
      " - MANTA_USDT_USDT-15m-futures.feather\n",
      " - MASA_USDT_USDT-15m-futures.feather\n",
      " - MASK_USDT_USDT-15m-futures.feather\n",
      " - MAV_USDT_USDT-15m-futures.feather\n",
      " - MAVIA_USDT_USDT-15m-futures.feather\n",
      " - MBL_USDT_USDT-15m-futures.feather\n",
      " - MBOX_USDT_USDT-15m-futures.feather\n",
      " - MDT_USDT_USDT-15m-futures.feather\n",
      " - ME_USDT_USDT-15m-futures.feather\n",
      " - MELANIA_USDT_USDT-15m-futures.feather\n",
      " - MEME_USDT_USDT-15m-futures.feather\n",
      " - MEMEFI_USDT_USDT-15m-futures.feather\n",
      " - MERL_USDT_USDT-15m-futures.feather\n",
      " - METIS_USDT_USDT-15m-futures.feather\n",
      " - MEW_USDT_USDT-15m-futures.feather\n",
      " - MICHI_USDT_USDT-15m-futures.feather\n",
      " - MINA_USDT_USDT-15m-futures.feather\n",
      " - MKR_USDT_USDT-15m-futures.feather\n",
      " - MLN_USDT_USDT-15m-futures.feather\n",
      " - MNT_USDT_USDT-15m-futures.feather\n",
      " - MOBILE_USDT_USDT-15m-futures.feather\n",
      " - MOCA_USDT_USDT-15m-futures.feather\n",
      " - MOODENG_USDT_USDT-15m-futures.feather\n",
      " - MORPHO_USDT_USDT-15m-futures.feather\n",
      " - MOVE_USDT_USDT-15m-futures.feather\n",
      " - MOVR_USDT_USDT-15m-futures.feather\n",
      " - MTL_USDT_USDT-15m-futures.feather\n",
      " - MUBARAK_USDT_USDT-15m-futures.feather\n",
      " - MVL_USDT_USDT-15m-futures.feather\n",
      " - MYRIA_USDT_USDT-15m-futures.feather\n",
      " - MYRO_USDT_USDT-15m-futures.feather\n",
      " - NC_USDT_USDT-15m-futures.feather\n",
      " - NEAR_USDT_USDT-15m-futures.feather\n",
      " - NEIROETH_USDT_USDT-15m-futures.feather\n",
      " - NEO_USDT_USDT-15m-futures.feather\n",
      " - NFP_USDT_USDT-15m-futures.feather\n",
      " - NIL_USDT_USDT-15m-futures.feather\n",
      " - NKN_USDT_USDT-15m-futures.feather\n",
      " - NMR_USDT_USDT-15m-futures.feather\n",
      " - NOT_USDT_USDT-15m-futures.feather\n",
      " - NS_USDT_USDT-15m-futures.feather\n",
      " - NTRN_USDT_USDT-15m-futures.feather\n",
      " - OBT_USDT_USDT-15m-futures.feather\n",
      " - OG_USDT_USDT-15m-futures.feather\n",
      " - OGN_USDT_USDT-15m-futures.feather\n",
      " - OL_USDT_USDT-15m-futures.feather\n",
      " - OM_USDT_USDT-15m-futures.feather\n",
      " - OMG_USDT_USDT-15m-futures.feather\n",
      " - OMNI_USDT_USDT-15m-futures.feather\n",
      " - ONDO_USDT_USDT-15m-futures.feather\n",
      " - ONE_USDT_USDT-15m-futures.feather\n",
      " - ONG_USDT_USDT-15m-futures.feather\n",
      " - ONT_USDT_USDT-15m-futures.feather\n",
      " - OP_USDT_USDT-15m-futures.feather\n",
      " - ORBS_USDT_USDT-15m-futures.feather\n",
      " - ORCA_USDT_USDT-15m-futures.feather\n",
      " - ORDER_USDT_USDT-15m-futures.feather\n",
      " - ORDI_USDT_USDT-15m-futures.feather\n",
      " - OSMO_USDT_USDT-15m-futures.feather\n",
      " - OXT_USDT_USDT-15m-futures.feather\n",
      " - PARTI_USDT_USDT-15m-futures.feather\n",
      " - PAXG_USDT_USDT-15m-futures.feather\n",
      " - PEAQ_USDT_USDT-15m-futures.feather\n",
      " - PENDLE_USDT_USDT-15m-futures.feather\n",
      " - PENGU_USDT_USDT-15m-futures.feather\n",
      " - PEOPLE_USDT_USDT-15m-futures.feather\n",
      " - PERP_USDT_USDT-15m-futures.feather\n",
      " - PHA_USDT_USDT-15m-futures.feather\n",
      " - PHB_USDT_USDT-15m-futures.feather\n",
      " - PIPPIN_USDT_USDT-15m-futures.feather\n",
      " - PIXEL_USDT_USDT-15m-futures.feather\n",
      " - PLUME_USDT_USDT-15m-futures.feather\n",
      " - PNUT_USDT_USDT-15m-futures.feather\n",
      " - POL_USDT_USDT-15m-futures.feather\n",
      " - POLYX_USDT_USDT-15m-futures.feather\n",
      " - PONKE_USDT_USDT-15m-futures.feather\n",
      " - POPCAT_USDT_USDT-15m-futures.feather\n",
      " - PORTAL_USDT_USDT-15m-futures.feather\n",
      " - POWR_USDT_USDT-15m-futures.feather\n",
      " - PRCL_USDT_USDT-15m-futures.feather\n",
      " - PRIME_USDT_USDT-15m-futures.feather\n",
      " - PROM_USDT_USDT-15m-futures.feather\n",
      " - PROMPT_USDT_USDT-15m-futures.feather\n",
      " - PUFFER_USDT_USDT-15m-futures.feather\n",
      " - PUMP_USDT_USDT-15m-futures.feather\n",
      " - PUNDIX_USDT_USDT-15m-futures.feather\n",
      " - PYR_USDT_USDT-15m-futures.feather\n",
      " - PYTH_USDT_USDT-15m-futures.feather\n",
      " - QI_USDT_USDT-15m-futures.feather\n",
      " - QNT_USDT_USDT-15m-futures.feather\n",
      " - QTUM_USDT_USDT-15m-futures.feather\n",
      " - QUICK_USDT_USDT-15m-futures.feather\n",
      " - RAD_USDT_USDT-15m-futures.feather\n",
      " - RARE_USDT_USDT-15m-futures.feather\n",
      " - RAYDIUM_USDT_USDT-15m-futures.feather\n",
      " - RDNT_USDT_USDT-15m-futures.feather\n",
      " - RED_USDT_USDT-15m-futures.feather\n",
      " - RENDER_USDT_USDT-15m-futures.feather\n",
      " - REQ_USDT_USDT-15m-futures.feather\n",
      " - REX_USDT_USDT-15m-futures.feather\n",
      " - REZ_USDT_USDT-15m-futures.feather\n",
      " - RFC_USDT_USDT-15m-futures.feather\n",
      " - RIF_USDT_USDT-15m-futures.feather\n",
      " - RLC_USDT_USDT-15m-futures.feather\n",
      " - ROAM_USDT_USDT-15m-futures.feather\n",
      " - RONIN_USDT_USDT-15m-futures.feather\n",
      " - ROSE_USDT_USDT-15m-futures.feather\n",
      " - RPL_USDT_USDT-15m-futures.feather\n",
      " - RSR_USDT_USDT-15m-futures.feather\n",
      " - RSS3_USDT_USDT-15m-futures.feather\n",
      " - RUNE_USDT_USDT-15m-futures.feather\n",
      " - RVN_USDT_USDT-15m-futures.feather\n",
      " - S_USDT_USDT-15m-futures.feather\n",
      " - SAFE_USDT_USDT-15m-futures.feather\n",
      " - SAGA_USDT_USDT-15m-futures.feather\n",
      " - SAND_USDT_USDT-15m-futures.feather\n",
      " - SC_USDT_USDT-15m-futures.feather\n",
      " - SCA_USDT_USDT-15m-futures.feather\n",
      " - SCR_USDT_USDT-15m-futures.feather\n",
      " - SCRT_USDT_USDT-15m-futures.feather\n",
      " - SD_USDT_USDT-15m-futures.feather\n",
      " - SEI_USDT_USDT-15m-futures.feather\n",
      " - SEND_USDT_USDT-15m-futures.feather\n",
      " - SERAPH_USDT_USDT-15m-futures.feather\n",
      " - SFP_USDT_USDT-15m-futures.feather\n",
      " - SHELL_USDT_USDT-15m-futures.feather\n",
      " - SHIB1000_USDT_USDT-15m-futures.feather\n",
      " - SIGN_USDT_USDT-15m-futures.feather\n",
      " - SIREN_USDT_USDT-15m-futures.feather\n",
      " - SKL_USDT_USDT-15m-futures.feather\n",
      " - SLERF_USDT_USDT-15m-futures.feather\n",
      " - SLF_USDT_USDT-15m-futures.feather\n",
      " - SLP_USDT_USDT-15m-futures.feather\n",
      " - SNT_USDT_USDT-15m-futures.feather\n",
      " - SNX_USDT_USDT-15m-futures.feather\n",
      " - SOL_USDT_USDT-15m-futures.feather\n",
      " - SOLAYER_USDT_USDT-15m-futures.feather\n",
      " - SOLO_USDT_USDT-15m-futures.feather\n",
      " - SOLV_USDT_USDT-15m-futures.feather\n",
      " - SONIC_USDT_USDT-15m-futures.feather\n",
      " - SPEC_USDT_USDT-15m-futures.feather\n",
      " - SPELL_USDT_USDT-15m-futures.feather\n",
      " - SPX_USDT_USDT-15m-futures.feather\n",
      " - SSV_USDT_USDT-15m-futures.feather\n",
      " - STEEM_USDT_USDT-15m-futures.feather\n",
      " - STG_USDT_USDT-15m-futures.feather\n",
      " - STO_USDT_USDT-15m-futures.feather\n",
      " - STORJ_USDT_USDT-15m-futures.feather\n",
      " - STPT_USDT_USDT-15m-futures.feather\n",
      " - STRK_USDT_USDT-15m-futures.feather\n",
      " - STX_USDT_USDT-15m-futures.feather\n",
      " - SUI_USDT_USDT-15m-futures.feather\n",
      " - SUN_USDT_USDT-15m-futures.feather\n",
      " - SUNDOG_USDT_USDT-15m-futures.feather\n",
      " - SUPER_USDT_USDT-15m-futures.feather\n",
      " - SUSHI_USDT_USDT-15m-futures.feather\n",
      " - SWARMS_USDT_USDT-15m-futures.feather\n",
      " - SWEAT_USDT_USDT-15m-futures.feather\n",
      " - SWELL_USDT_USDT-15m-futures.feather\n",
      " - SXP_USDT_USDT-15m-futures.feather\n",
      " - SYN_USDT_USDT-15m-futures.feather\n",
      " - SYS_USDT_USDT-15m-futures.feather\n",
      " - T_USDT_USDT-15m-futures.feather\n",
      " - TAI_USDT_USDT-15m-futures.feather\n",
      " - TAIKO_USDT_USDT-15m-futures.feather\n",
      " - TAO_USDT_USDT-15m-futures.feather\n",
      " - THE_USDT_USDT-15m-futures.feather\n",
      " - THETA_USDT_USDT-15m-futures.feather\n",
      " - TIA_USDT_USDT-15m-futures.feather\n",
      " - TLM_USDT_USDT-15m-futures.feather\n",
      " - TNSR_USDT_USDT-15m-futures.feather\n",
      " - TOKEN_USDT_USDT-15m-futures.feather\n",
      " - TON_USDT_USDT-15m-futures.feather\n",
      " - TRB_USDT_USDT-15m-futures.feather\n",
      " - TRU_USDT_USDT-15m-futures.feather\n",
      " - TRUMP_USDT_USDT-15m-futures.feather\n",
      " - TRX_USDT_USDT-15m-futures.feather\n",
      " - TSTBSC_USDT_USDT-15m-futures.feather\n",
      " - TUT_USDT_USDT-15m-futures.feather\n",
      " - TWT_USDT_USDT-15m-futures.feather\n",
      " - UMA_USDT_USDT-15m-futures.feather\n",
      " - UNI_USDT_USDT-15m-futures.feather\n",
      " - USDC_USDT_USDT-15m-futures.feather\n",
      " - USDE_USDT_USDT-15m-futures.feather\n",
      " - USTC_USDT_USDT-15m-futures.feather\n",
      " - USUAL_USDT_USDT-15m-futures.feather\n",
      " - UXLINK_USDT_USDT-15m-futures.feather\n",
      " - VANA_USDT_USDT-15m-futures.feather\n",
      " - VANRY_USDT_USDT-15m-futures.feather\n",
      " - VELO_USDT_USDT-15m-futures.feather\n",
      " - VELODROME_USDT_USDT-15m-futures.feather\n",
      " - VET_USDT_USDT-15m-futures.feather\n",
      " - VIC_USDT_USDT-15m-futures.feather\n",
      " - VINE_USDT_USDT-15m-futures.feather\n",
      " - VIRTUAL_USDT_USDT-15m-futures.feather\n",
      " - VOXEL_USDT_USDT-15m-futures.feather\n",
      " - VR_USDT_USDT-15m-futures.feather\n",
      " - VRA_USDT_USDT-15m-futures.feather\n",
      " - VTHO_USDT_USDT-15m-futures.feather\n",
      " - VVV_USDT_USDT-15m-futures.feather\n",
      " - W_USDT_USDT-15m-futures.feather\n",
      " - WAL_USDT_USDT-15m-futures.feather\n",
      " - WAVES_USDT_USDT-15m-futures.feather\n",
      " - WAXP_USDT_USDT-15m-futures.feather\n",
      " - WCT_USDT_USDT-15m-futures.feather\n",
      " - WIF_USDT_USDT-15m-futures.feather\n",
      " - WLD_USDT_USDT-15m-futures.feather\n",
      " - WOO_USDT_USDT-15m-futures.feather\n",
      " - XAI_USDT_USDT-15m-futures.feather\n",
      " - XAUT_USDT_USDT-15m-futures.feather\n",
      " - XCH_USDT_USDT-15m-futures.feather\n",
      " - XCN_USDT_USDT-15m-futures.feather\n",
      " - XDC_USDT_USDT-15m-futures.feather\n",
      " - XEM_USDT_USDT-15m-futures.feather\n",
      " - XION_USDT_USDT-15m-futures.feather\n",
      " - XLM_USDT_USDT-15m-futures.feather\n",
      " - XMR_USDT_USDT-15m-futures.feather\n",
      " - XNO_USDT_USDT-15m-futures.feather\n",
      " - XRD_USDT_USDT-15m-futures.feather\n",
      " - XRP_USDT_USDT-15m-futures.feather\n",
      " - XTER_USDT_USDT-15m-futures.feather\n",
      " - XTZ_USDT_USDT-15m-futures.feather\n",
      " - XVG_USDT_USDT-15m-futures.feather\n",
      " - XVS_USDT_USDT-15m-futures.feather\n",
      " - YFI_USDT_USDT-15m-futures.feather\n",
      " - YGG_USDT_USDT-15m-futures.feather\n",
      " - ZBCN_USDT_USDT-15m-futures.feather\n",
      " - ZEC_USDT_USDT-15m-futures.feather\n",
      " - ZEN_USDT_USDT-15m-futures.feather\n",
      " - ZENT_USDT_USDT-15m-futures.feather\n",
      " - ZEREBRO_USDT_USDT-15m-futures.feather\n",
      " - ZETA_USDT_USDT-15m-futures.feather\n",
      " - ZEUS_USDT_USDT-15m-futures.feather\n",
      " - ZIL_USDT_USDT-15m-futures.feather\n",
      " - ZK_USDT_USDT-15m-futures.feather\n",
      " - ZKJ_USDT_USDT-15m-futures.feather\n",
      " - ZORA_USDT_USDT-15m-futures.feather\n",
      " - ZRC_USDT_USDT-15m-futures.feather\n",
      " - ZRO_USDT_USDT-15m-futures.feather\n",
      " - ZRX_USDT_USDT-15m-futures.feather\n",
      "\n",
      "==================== Starting Parallel Asset Processing (14 jobs) ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# @title Main Execution: Discover Files and Run in Parallel\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Find Asset Files ---\n",
    "    asset_files_to_process = []\n",
    "    try:\n",
    "        all_files_in_dir = os.listdir(config_params[\"datasets_dir\"])\n",
    "        for filename_main in all_files_in_dir:\n",
    "            if filename_main.endswith('.feather') and f'-{config_params[\"frequency\"]}-futures' in filename_main:\n",
    "                asset_files_to_process.append(os.path.join(config_params[\"datasets_dir\"], filename_main))\n",
    "    except FileNotFoundError: print(f\"Error: Datasets directory not found: {config_params['datasets_dir']}\")\n",
    "    except Exception as e: print(f\"Error listing files: {e}\")\n",
    "\n",
    "    if not asset_files_to_process:\n",
    "        print(f\"No '.feather' files found for frequency '{config_params['frequency']}'. Exiting.\")\n",
    "    else:\n",
    "        print(f\"\\nFound {len(asset_files_to_process)} asset files to process:\")\n",
    "        # asset_files_to_process = asset_files_to_process[:1] # Limit for testing\n",
    "        # print(f\">>> Limiting to first {len(asset_files_to_process)} file(s) for testing <<<\")\n",
    "        for f_main in asset_files_to_process: print(f\" - {os.path.basename(f_main)}\")\n",
    "\n",
    "        print(f\"\\n{'='*20} Starting Parallel Asset Processing ({num_concurrent_runs} jobs) {'='*20}\")\n",
    "        start_parallel_time = time.time()\n",
    "\n",
    "        results_list_parallel = Parallel(n_jobs=num_concurrent_runs, backend=\"loky\", verbose=10)(\n",
    "             delayed(process_asset)(file_path, config_params) for file_path in asset_files_to_process\n",
    "        )\n",
    "\n",
    "        end_parallel_time = time.time()\n",
    "        print(f\"\\n{'='*20} Parallel Asset Processing Finished {'='*20}\")\n",
    "        print(f\"Total time for parallel execution: {(end_parallel_time - start_parallel_time)/60:.2f} minutes\")\n",
    "\n",
    "        print(\"\\nAggregating results from parallel jobs...\")\n",
    "        all_assets_all_targets_results_flat = []\n",
    "        job_success_count = 0; job_fail_count = 0\n",
    "        if results_list_parallel:\n",
    "            for single_asset_results_list in results_list_parallel:\n",
    "                if isinstance(single_asset_results_list, list) and single_asset_results_list:\n",
    "                    all_assets_all_targets_results_flat.extend(single_asset_results_list)\n",
    "                    job_success_count +=1\n",
    "                elif isinstance(single_asset_results_list, list) and not single_asset_results_list:\n",
    "                    job_fail_count +=1\n",
    "                else:\n",
    "                    print(f\"Warning: A parallel job did not return a list of results. Result: {type(single_asset_results_list)}\")\n",
    "                    job_fail_count +=1\n",
    "        print(f\"Aggregation complete. Total results records: {len(all_assets_all_targets_results_flat)}\")\n",
    "        print(f\"Assets processed (at least one target run): {job_success_count}\")\n",
    "        print(f\"Assets potentially failed or skipped entirely: {job_fail_count}\")\n",
    "\n",
    "        if all_assets_all_targets_results_flat:\n",
    "             all_results_save_path = os.path.join(config_params[\"results_dir_timedated\"], f'{config_params[\"date_today\"]}_{config_params[\"frequency\"]}_ALL_ASSETS_TARGETS_healthspan_summary.json')\n",
    "             try:\n",
    "                  serializable_all_results_flat = []\n",
    "                  for record in all_assets_all_targets_results_flat:\n",
    "                      new_record = record.copy(); temp_scores = []\n",
    "                      if 'external_eval_wql_scores' in new_record and new_record['external_eval_wql_scores']:\n",
    "                          for score_item in new_record['external_eval_wql_scores']:\n",
    "                               new_score_item = score_item.copy()\n",
    "                               time_delta = new_score_item.get('time_since_train')\n",
    "                               if isinstance(time_delta, pd.Timedelta): new_score_item['time_since_train'] = None if pd.isna(time_delta) else time_delta.total_seconds()\n",
    "                               temp_scores.append(new_score_item)\n",
    "                          new_record['external_eval_wql_scores'] = temp_scores\n",
    "                      serializable_all_results_flat.append(new_record)\n",
    "                  with open(all_results_save_path, 'w') as f: json.dump(serializable_all_results_flat, f, default=default_serializer, indent=4)\n",
    "                  print(f\"\\nCOMBINED detailed results summary saved to JSON: {all_results_save_path}\")\n",
    "             except Exception as e: print(f\"\\nError saving COMBINED results summary: {e}\")\n",
    "        else: print(\"\\nNo results generated across all assets to save.\")\n",
    "\n",
    "    # if os.path.exists(local_temp_models_base):\n",
    "    #     print(f\"\\nCleaning up base temporary model directory: {local_temp_models_base}\")\n",
    "    #     try: shutil.rmtree(local_temp_models_base); print(f\"Successfully removed {local_temp_models_base}\")\n",
    "    #     except OSError as e: print(f\"Error removing {local_temp_models_base}: {e}\")\n",
    "    # else: print(f\"\\nBase temporary model directory {local_temp_models_base} not found, no cleanup needed or already cleaned.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
